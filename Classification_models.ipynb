{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт всех нужных библиотек\n",
    "\n",
    "import pandas as pd \n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.metrics  import classification_report  \n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.svm import SVC \n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "# Ансамбли\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "# Линейный модели\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import make_classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модели\n",
    "\n",
    "MODELS ={\n",
    "    #'DummyClassifier': DummyClassifier(),\n",
    "    \n",
    "    'AdaBoostClassifier': AdaBoostClassifier(),\n",
    "    'BaggingClassifier': BaggingClassifier(),\n",
    "    'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'RandomForestClassifier_default': RandomForestClassifier(),\n",
    "    'RandomForestClassifier_params': RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    \n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'LogisticRegressionCV': LogisticRegressionCV(),\n",
    "    'PassiveAggressiveClassifier': PassiveAggressiveClassifier(),\n",
    "    'RidgeClassifier': RidgeClassifier(),\n",
    "    'RidgeClassifierCV': RidgeClassifierCV(),\n",
    "    'SGDClassifier': SGDClassifier(),\n",
    "    \n",
    "    'MLPClassifier': MLPClassifier(),\n",
    "    \n",
    "    # 'GaussianProcessClassifier': GaussianProcessClassifier(),\n",
    "    \n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'RadiusNeighborsClassifier': RadiusNeighborsClassifier(),\n",
    "    \n",
    "    'LinearSVC': LinearSVC(),\n",
    "    'NuSVC': NuSVC(),\n",
    "    'SVC': SVC(),\n",
    "    \n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'ExtraTreeClassifier': ExtraTreeClassifier(),\n",
    "    \n",
    "    'GaussianProcessClassifier': GaussianProcessClassifier(),\n",
    "    \n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    \n",
    "    'LinearDiscriminantAnalysis': LinearDiscriminantAnalysis(),\n",
    "    'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis()\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X291</th>\n",
       "      <th>X292</th>\n",
       "      <th>X293</th>\n",
       "      <th>X294</th>\n",
       "      <th>X295</th>\n",
       "      <th>X296</th>\n",
       "      <th>X297</th>\n",
       "      <th>X298</th>\n",
       "      <th>X299</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.833715</td>\n",
       "      <td>-0.059713</td>\n",
       "      <td>0.335309</td>\n",
       "      <td>-0.974730</td>\n",
       "      <td>-0.841654</td>\n",
       "      <td>1.852721</td>\n",
       "      <td>-0.705961</td>\n",
       "      <td>-1.971906</td>\n",
       "      <td>0.202016</td>\n",
       "      <td>0.184296</td>\n",
       "      <td>...</td>\n",
       "      <td>1.502413</td>\n",
       "      <td>0.412222</td>\n",
       "      <td>-0.075524</td>\n",
       "      <td>0.223902</td>\n",
       "      <td>0.274205</td>\n",
       "      <td>0.415715</td>\n",
       "      <td>-0.599985</td>\n",
       "      <td>-1.304587</td>\n",
       "      <td>-2.774132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.275810</td>\n",
       "      <td>1.124346</td>\n",
       "      <td>1.549267</td>\n",
       "      <td>0.677528</td>\n",
       "      <td>1.324331</td>\n",
       "      <td>-0.820775</td>\n",
       "      <td>1.845078</td>\n",
       "      <td>-0.674491</td>\n",
       "      <td>-1.464548</td>\n",
       "      <td>-0.468788</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.443428</td>\n",
       "      <td>0.724280</td>\n",
       "      <td>1.768952</td>\n",
       "      <td>1.145024</td>\n",
       "      <td>0.775345</td>\n",
       "      <td>-0.879923</td>\n",
       "      <td>0.291254</td>\n",
       "      <td>-0.819193</td>\n",
       "      <td>2.514119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.387955</td>\n",
       "      <td>0.903770</td>\n",
       "      <td>-1.154365</td>\n",
       "      <td>1.201128</td>\n",
       "      <td>-2.708176</td>\n",
       "      <td>-1.248790</td>\n",
       "      <td>-1.104851</td>\n",
       "      <td>-1.910245</td>\n",
       "      <td>2.325045</td>\n",
       "      <td>0.132566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215878</td>\n",
       "      <td>0.660136</td>\n",
       "      <td>-0.192481</td>\n",
       "      <td>1.083527</td>\n",
       "      <td>0.214616</td>\n",
       "      <td>-0.435041</td>\n",
       "      <td>-0.972098</td>\n",
       "      <td>2.137916</td>\n",
       "      <td>0.900635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.136877</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.167749</td>\n",
       "      <td>0.107469</td>\n",
       "      <td>1.486963</td>\n",
       "      <td>-0.952085</td>\n",
       "      <td>-1.360380</td>\n",
       "      <td>0.268054</td>\n",
       "      <td>0.688296</td>\n",
       "      <td>-0.610165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996178</td>\n",
       "      <td>-1.312817</td>\n",
       "      <td>-0.700641</td>\n",
       "      <td>1.366988</td>\n",
       "      <td>1.013384</td>\n",
       "      <td>-0.703875</td>\n",
       "      <td>1.246894</td>\n",
       "      <td>-0.052056</td>\n",
       "      <td>-2.478722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.217964</td>\n",
       "      <td>0.196984</td>\n",
       "      <td>0.529772</td>\n",
       "      <td>0.197816</td>\n",
       "      <td>0.791958</td>\n",
       "      <td>0.039262</td>\n",
       "      <td>0.152459</td>\n",
       "      <td>1.553762</td>\n",
       "      <td>0.337509</td>\n",
       "      <td>1.227309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387539</td>\n",
       "      <td>-0.074095</td>\n",
       "      <td>0.432393</td>\n",
       "      <td>-0.599696</td>\n",
       "      <td>-2.118808</td>\n",
       "      <td>2.213384</td>\n",
       "      <td>0.680090</td>\n",
       "      <td>0.494825</td>\n",
       "      <td>-0.169699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>1.075133</td>\n",
       "      <td>-0.255920</td>\n",
       "      <td>-1.036469</td>\n",
       "      <td>-0.275893</td>\n",
       "      <td>-0.476321</td>\n",
       "      <td>-0.314835</td>\n",
       "      <td>-0.882762</td>\n",
       "      <td>0.619231</td>\n",
       "      <td>-1.342816</td>\n",
       "      <td>1.180036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379811</td>\n",
       "      <td>0.339671</td>\n",
       "      <td>-0.009594</td>\n",
       "      <td>-2.056156</td>\n",
       "      <td>0.278989</td>\n",
       "      <td>-1.373832</td>\n",
       "      <td>-0.208763</td>\n",
       "      <td>1.431086</td>\n",
       "      <td>-0.542631</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>-0.072280</td>\n",
       "      <td>0.898810</td>\n",
       "      <td>1.563341</td>\n",
       "      <td>0.419205</td>\n",
       "      <td>1.170910</td>\n",
       "      <td>-0.729022</td>\n",
       "      <td>0.207427</td>\n",
       "      <td>0.540910</td>\n",
       "      <td>0.407059</td>\n",
       "      <td>0.320830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432756</td>\n",
       "      <td>2.053451</td>\n",
       "      <td>-0.258599</td>\n",
       "      <td>-0.326833</td>\n",
       "      <td>0.827689</td>\n",
       "      <td>1.234338</td>\n",
       "      <td>-0.167094</td>\n",
       "      <td>1.492528</td>\n",
       "      <td>-0.791885</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>-1.090896</td>\n",
       "      <td>1.143751</td>\n",
       "      <td>-0.823059</td>\n",
       "      <td>0.159191</td>\n",
       "      <td>0.263572</td>\n",
       "      <td>-1.232017</td>\n",
       "      <td>0.045819</td>\n",
       "      <td>0.525129</td>\n",
       "      <td>1.707128</td>\n",
       "      <td>-0.315056</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609726</td>\n",
       "      <td>0.536863</td>\n",
       "      <td>0.878395</td>\n",
       "      <td>-0.273163</td>\n",
       "      <td>0.693581</td>\n",
       "      <td>-1.766539</td>\n",
       "      <td>0.030766</td>\n",
       "      <td>-0.103282</td>\n",
       "      <td>-0.688002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>1.047330</td>\n",
       "      <td>-0.806229</td>\n",
       "      <td>-1.099473</td>\n",
       "      <td>-1.359373</td>\n",
       "      <td>1.709767</td>\n",
       "      <td>-0.982627</td>\n",
       "      <td>0.546173</td>\n",
       "      <td>0.525552</td>\n",
       "      <td>0.869776</td>\n",
       "      <td>-0.393150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368007</td>\n",
       "      <td>1.231185</td>\n",
       "      <td>0.295251</td>\n",
       "      <td>0.270774</td>\n",
       "      <td>0.615555</td>\n",
       "      <td>0.679398</td>\n",
       "      <td>0.857667</td>\n",
       "      <td>0.907869</td>\n",
       "      <td>0.293574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0.525668</td>\n",
       "      <td>-0.930143</td>\n",
       "      <td>-0.113496</td>\n",
       "      <td>1.941856</td>\n",
       "      <td>1.592205</td>\n",
       "      <td>0.683930</td>\n",
       "      <td>-0.171717</td>\n",
       "      <td>1.633509</td>\n",
       "      <td>-0.559569</td>\n",
       "      <td>-1.224737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116997</td>\n",
       "      <td>1.280165</td>\n",
       "      <td>-1.757208</td>\n",
       "      <td>-0.119389</td>\n",
       "      <td>-0.696702</td>\n",
       "      <td>0.377051</td>\n",
       "      <td>-0.007454</td>\n",
       "      <td>0.406353</td>\n",
       "      <td>0.108463</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X0        X1        X2        X3        X4        X5        X6  \\\n",
       "0    -0.833715 -0.059713  0.335309 -0.974730 -0.841654  1.852721 -0.705961   \n",
       "1    -0.275810  1.124346  1.549267  0.677528  1.324331 -0.820775  1.845078   \n",
       "2     0.387955  0.903770 -1.154365  1.201128 -2.708176 -1.248790 -1.104851   \n",
       "3    -1.136877  0.821979  0.167749  0.107469  1.486963 -0.952085 -1.360380   \n",
       "4     1.217964  0.196984  0.529772  0.197816  0.791958  0.039262  0.152459   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2995  1.075133 -0.255920 -1.036469 -0.275893 -0.476321 -0.314835 -0.882762   \n",
       "2996 -0.072280  0.898810  1.563341  0.419205  1.170910 -0.729022  0.207427   \n",
       "2997 -1.090896  1.143751 -0.823059  0.159191  0.263572 -1.232017  0.045819   \n",
       "2998  1.047330 -0.806229 -1.099473 -1.359373  1.709767 -0.982627  0.546173   \n",
       "2999  0.525668 -0.930143 -0.113496  1.941856  1.592205  0.683930 -0.171717   \n",
       "\n",
       "            X7        X8        X9  ...      X291      X292      X293  \\\n",
       "0    -1.971906  0.202016  0.184296  ...  1.502413  0.412222 -0.075524   \n",
       "1    -0.674491 -1.464548 -0.468788  ... -0.443428  0.724280  1.768952   \n",
       "2    -1.910245  2.325045  0.132566  ... -0.215878  0.660136 -0.192481   \n",
       "3     0.268054  0.688296 -0.610165  ...  0.996178 -1.312817 -0.700641   \n",
       "4     1.553762  0.337509  1.227309  ...  0.387539 -0.074095  0.432393   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2995  0.619231 -1.342816  1.180036  ... -0.379811  0.339671 -0.009594   \n",
       "2996  0.540910  0.407059  0.320830  ...  0.432756  2.053451 -0.258599   \n",
       "2997  0.525129  1.707128 -0.315056  ...  1.609726  0.536863  0.878395   \n",
       "2998  0.525552  0.869776 -0.393150  ...  0.368007  1.231185  0.295251   \n",
       "2999  1.633509 -0.559569 -1.224737  ...  0.116997  1.280165 -1.757208   \n",
       "\n",
       "          X294      X295      X296      X297      X298      X299  Y  \n",
       "0     0.223902  0.274205  0.415715 -0.599985 -1.304587 -2.774132  1  \n",
       "1     1.145024  0.775345 -0.879923  0.291254 -0.819193  2.514119  1  \n",
       "2     1.083527  0.214616 -0.435041 -0.972098  2.137916  0.900635  1  \n",
       "3     1.366988  1.013384 -0.703875  1.246894 -0.052056 -2.478722  0  \n",
       "4    -0.599696 -2.118808  2.213384  0.680090  0.494825 -0.169699  1  \n",
       "...        ...       ...       ...       ...       ...       ... ..  \n",
       "2995 -2.056156  0.278989 -1.373832 -0.208763  1.431086 -0.542631  1  \n",
       "2996 -0.326833  0.827689  1.234338 -0.167094  1.492528 -0.791885  0  \n",
       "2997 -0.273163  0.693581 -1.766539  0.030766 -0.103282 -0.688002  1  \n",
       "2998  0.270774  0.615555  0.679398  0.857667  0.907869  0.293574  1  \n",
       "2999 -0.119389 -0.696702  0.377051 -0.007454  0.406353  0.108463  0  \n",
       "\n",
       "[3000 rows x 301 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Формируем данные исходных dataset\n",
    "\n",
    "X, Y = make_classification(n_samples = 3000, n_features=300)\n",
    "X = pd.DataFrame(X)\n",
    "Y = pd.DataFrame(Y)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "LIST_NAME_FEATURES = []\n",
    "LIST_NAME_Y = []\n",
    "\n",
    "# Фичи\n",
    "for index, name in enumerate(X.columns):\n",
    "    LIST_NAME_FEATURES.append(f\"X{index}\")\n",
    "    df[f\"X{index}\"] = X[index]\n",
    "    \n",
    "# Y\n",
    "for index, name in enumerate(Y.columns):\n",
    "    LIST_NAME_Y.append(f\"Y\")\n",
    "    df[f\"Y\"] = Y[index]\n",
    "\n",
    "# Убираем nan\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X184', 'X13']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X291</th>\n",
       "      <th>X292</th>\n",
       "      <th>X293</th>\n",
       "      <th>X294</th>\n",
       "      <th>X295</th>\n",
       "      <th>X296</th>\n",
       "      <th>X297</th>\n",
       "      <th>X298</th>\n",
       "      <th>X299</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.833715</td>\n",
       "      <td>-0.059713</td>\n",
       "      <td>0.335309</td>\n",
       "      <td>-0.974730</td>\n",
       "      <td>-0.841654</td>\n",
       "      <td>1.852721</td>\n",
       "      <td>-0.705961</td>\n",
       "      <td>-1.971906</td>\n",
       "      <td>0.202016</td>\n",
       "      <td>0.184296</td>\n",
       "      <td>...</td>\n",
       "      <td>1.502413</td>\n",
       "      <td>0.412222</td>\n",
       "      <td>-0.075524</td>\n",
       "      <td>0.223902</td>\n",
       "      <td>0.274205</td>\n",
       "      <td>0.415715</td>\n",
       "      <td>-0.599985</td>\n",
       "      <td>-1.304587</td>\n",
       "      <td>-2.774132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.275810</td>\n",
       "      <td>1.124346</td>\n",
       "      <td>1.549267</td>\n",
       "      <td>0.677528</td>\n",
       "      <td>1.324331</td>\n",
       "      <td>-0.820775</td>\n",
       "      <td>1.845078</td>\n",
       "      <td>-0.674491</td>\n",
       "      <td>-1.464548</td>\n",
       "      <td>-0.468788</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.443428</td>\n",
       "      <td>0.724280</td>\n",
       "      <td>1.768952</td>\n",
       "      <td>1.145024</td>\n",
       "      <td>0.775345</td>\n",
       "      <td>-0.879923</td>\n",
       "      <td>0.291254</td>\n",
       "      <td>-0.819193</td>\n",
       "      <td>2.514119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.387955</td>\n",
       "      <td>0.903770</td>\n",
       "      <td>-1.154365</td>\n",
       "      <td>1.201128</td>\n",
       "      <td>-2.708176</td>\n",
       "      <td>-1.248790</td>\n",
       "      <td>-1.104851</td>\n",
       "      <td>-1.910245</td>\n",
       "      <td>2.325045</td>\n",
       "      <td>0.132566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215878</td>\n",
       "      <td>0.660136</td>\n",
       "      <td>-0.192481</td>\n",
       "      <td>1.083527</td>\n",
       "      <td>0.214616</td>\n",
       "      <td>-0.435041</td>\n",
       "      <td>-0.972098</td>\n",
       "      <td>2.137916</td>\n",
       "      <td>0.900635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.136877</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.167749</td>\n",
       "      <td>0.107469</td>\n",
       "      <td>1.486963</td>\n",
       "      <td>-0.952085</td>\n",
       "      <td>-1.360380</td>\n",
       "      <td>0.268054</td>\n",
       "      <td>0.688296</td>\n",
       "      <td>-0.610165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996178</td>\n",
       "      <td>-1.312817</td>\n",
       "      <td>-0.700641</td>\n",
       "      <td>1.366988</td>\n",
       "      <td>1.013384</td>\n",
       "      <td>-0.703875</td>\n",
       "      <td>1.246894</td>\n",
       "      <td>-0.052056</td>\n",
       "      <td>-2.478722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.217964</td>\n",
       "      <td>0.196984</td>\n",
       "      <td>0.529772</td>\n",
       "      <td>0.197816</td>\n",
       "      <td>0.791958</td>\n",
       "      <td>0.039262</td>\n",
       "      <td>0.152459</td>\n",
       "      <td>1.553762</td>\n",
       "      <td>0.337509</td>\n",
       "      <td>1.227309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387539</td>\n",
       "      <td>-0.074095</td>\n",
       "      <td>0.432393</td>\n",
       "      <td>-0.599696</td>\n",
       "      <td>-2.118808</td>\n",
       "      <td>2.213384</td>\n",
       "      <td>0.680090</td>\n",
       "      <td>0.494825</td>\n",
       "      <td>-0.169699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>1.075133</td>\n",
       "      <td>-0.255920</td>\n",
       "      <td>-1.036469</td>\n",
       "      <td>-0.275893</td>\n",
       "      <td>-0.476321</td>\n",
       "      <td>-0.314835</td>\n",
       "      <td>-0.882762</td>\n",
       "      <td>0.619231</td>\n",
       "      <td>-1.342816</td>\n",
       "      <td>1.180036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379811</td>\n",
       "      <td>0.339671</td>\n",
       "      <td>-0.009594</td>\n",
       "      <td>-2.056156</td>\n",
       "      <td>0.278989</td>\n",
       "      <td>-1.373832</td>\n",
       "      <td>-0.208763</td>\n",
       "      <td>1.431086</td>\n",
       "      <td>-0.542631</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>-0.072280</td>\n",
       "      <td>0.898810</td>\n",
       "      <td>1.563341</td>\n",
       "      <td>0.419205</td>\n",
       "      <td>1.170910</td>\n",
       "      <td>-0.729022</td>\n",
       "      <td>0.207427</td>\n",
       "      <td>0.540910</td>\n",
       "      <td>0.407059</td>\n",
       "      <td>0.320830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432756</td>\n",
       "      <td>2.053451</td>\n",
       "      <td>-0.258599</td>\n",
       "      <td>-0.326833</td>\n",
       "      <td>0.827689</td>\n",
       "      <td>1.234338</td>\n",
       "      <td>-0.167094</td>\n",
       "      <td>1.492528</td>\n",
       "      <td>-0.791885</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>-1.090896</td>\n",
       "      <td>1.143751</td>\n",
       "      <td>-0.823059</td>\n",
       "      <td>0.159191</td>\n",
       "      <td>0.263572</td>\n",
       "      <td>-1.232017</td>\n",
       "      <td>0.045819</td>\n",
       "      <td>0.525129</td>\n",
       "      <td>1.707128</td>\n",
       "      <td>-0.315056</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609726</td>\n",
       "      <td>0.536863</td>\n",
       "      <td>0.878395</td>\n",
       "      <td>-0.273163</td>\n",
       "      <td>0.693581</td>\n",
       "      <td>-1.766539</td>\n",
       "      <td>0.030766</td>\n",
       "      <td>-0.103282</td>\n",
       "      <td>-0.688002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>1.047330</td>\n",
       "      <td>-0.806229</td>\n",
       "      <td>-1.099473</td>\n",
       "      <td>-1.359373</td>\n",
       "      <td>1.709767</td>\n",
       "      <td>-0.982627</td>\n",
       "      <td>0.546173</td>\n",
       "      <td>0.525552</td>\n",
       "      <td>0.869776</td>\n",
       "      <td>-0.393150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368007</td>\n",
       "      <td>1.231185</td>\n",
       "      <td>0.295251</td>\n",
       "      <td>0.270774</td>\n",
       "      <td>0.615555</td>\n",
       "      <td>0.679398</td>\n",
       "      <td>0.857667</td>\n",
       "      <td>0.907869</td>\n",
       "      <td>0.293574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0.525668</td>\n",
       "      <td>-0.930143</td>\n",
       "      <td>-0.113496</td>\n",
       "      <td>1.941856</td>\n",
       "      <td>1.592205</td>\n",
       "      <td>0.683930</td>\n",
       "      <td>-0.171717</td>\n",
       "      <td>1.633509</td>\n",
       "      <td>-0.559569</td>\n",
       "      <td>-1.224737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116997</td>\n",
       "      <td>1.280165</td>\n",
       "      <td>-1.757208</td>\n",
       "      <td>-0.119389</td>\n",
       "      <td>-0.696702</td>\n",
       "      <td>0.377051</td>\n",
       "      <td>-0.007454</td>\n",
       "      <td>0.406353</td>\n",
       "      <td>0.108463</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 299 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X0        X1        X2        X3        X4        X5        X6  \\\n",
       "0    -0.833715 -0.059713  0.335309 -0.974730 -0.841654  1.852721 -0.705961   \n",
       "1    -0.275810  1.124346  1.549267  0.677528  1.324331 -0.820775  1.845078   \n",
       "2     0.387955  0.903770 -1.154365  1.201128 -2.708176 -1.248790 -1.104851   \n",
       "3    -1.136877  0.821979  0.167749  0.107469  1.486963 -0.952085 -1.360380   \n",
       "4     1.217964  0.196984  0.529772  0.197816  0.791958  0.039262  0.152459   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2995  1.075133 -0.255920 -1.036469 -0.275893 -0.476321 -0.314835 -0.882762   \n",
       "2996 -0.072280  0.898810  1.563341  0.419205  1.170910 -0.729022  0.207427   \n",
       "2997 -1.090896  1.143751 -0.823059  0.159191  0.263572 -1.232017  0.045819   \n",
       "2998  1.047330 -0.806229 -1.099473 -1.359373  1.709767 -0.982627  0.546173   \n",
       "2999  0.525668 -0.930143 -0.113496  1.941856  1.592205  0.683930 -0.171717   \n",
       "\n",
       "            X7        X8        X9  ...      X291      X292      X293  \\\n",
       "0    -1.971906  0.202016  0.184296  ...  1.502413  0.412222 -0.075524   \n",
       "1    -0.674491 -1.464548 -0.468788  ... -0.443428  0.724280  1.768952   \n",
       "2    -1.910245  2.325045  0.132566  ... -0.215878  0.660136 -0.192481   \n",
       "3     0.268054  0.688296 -0.610165  ...  0.996178 -1.312817 -0.700641   \n",
       "4     1.553762  0.337509  1.227309  ...  0.387539 -0.074095  0.432393   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2995  0.619231 -1.342816  1.180036  ... -0.379811  0.339671 -0.009594   \n",
       "2996  0.540910  0.407059  0.320830  ...  0.432756  2.053451 -0.258599   \n",
       "2997  0.525129  1.707128 -0.315056  ...  1.609726  0.536863  0.878395   \n",
       "2998  0.525552  0.869776 -0.393150  ...  0.368007  1.231185  0.295251   \n",
       "2999  1.633509 -0.559569 -1.224737  ...  0.116997  1.280165 -1.757208   \n",
       "\n",
       "          X294      X295      X296      X297      X298      X299  Y  \n",
       "0     0.223902  0.274205  0.415715 -0.599985 -1.304587 -2.774132  1  \n",
       "1     1.145024  0.775345 -0.879923  0.291254 -0.819193  2.514119  1  \n",
       "2     1.083527  0.214616 -0.435041 -0.972098  2.137916  0.900635  1  \n",
       "3     1.366988  1.013384 -0.703875  1.246894 -0.052056 -2.478722  0  \n",
       "4    -0.599696 -2.118808  2.213384  0.680090  0.494825 -0.169699  1  \n",
       "...        ...       ...       ...       ...       ...       ... ..  \n",
       "2995 -2.056156  0.278989 -1.373832 -0.208763  1.431086 -0.542631  1  \n",
       "2996 -0.326833  0.827689  1.234338 -0.167094  1.492528 -0.791885  0  \n",
       "2997 -0.273163  0.693581 -1.766539  0.030766 -0.103282 -0.688002  1  \n",
       "2998  0.270774  0.615555  0.679398  0.857667  0.907869  0.293574  1  \n",
       "2999 -0.119389 -0.696702  0.377051 -0.007454  0.406353  0.108463  0  \n",
       "\n",
       "[3000 rows x 299 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Функция для проверки на корреляцию\n",
    "def valid_corr(df, max_corr=0.8):\n",
    "    \n",
    "    # Матрица корреляции элментов\n",
    "    matrix_corr = df.corr()\n",
    "\n",
    "    # Список для хранения параметров, которые буде удалять\n",
    "    corr_list = []\n",
    "\n",
    "    # Добавим в наш список все элеементы корреляция коорых между собой нас не устраивет\n",
    "    for i in range(1, len(matrix_corr)):\n",
    "        for j in range(i + 1, len(matrix_corr) - 1):\n",
    "            #print(f'{f\"X{i}\"} - {f\"X{j}\"} >>> {abs(matrix_corr[f\"X{i}\"][f\"X{j}\"])}')\n",
    "            # Если значение корреляции больше, чем допуспимое значение\n",
    "            if abs(matrix_corr[f\"X{i}\"][f\"X{j}\"]) > max_corr:\n",
    "                \n",
    "                # Смотрим какой из параметров меньше коррелирует с 'y' и добавляем его в список для дальнейшего удаления\n",
    "                if matrix_corr[f\"X{i}\"][\"Y\"] > matrix_corr[f\"X{j}\"][\"Y\"]:\n",
    "                    corr_list.append(f\"X{j}\")\n",
    "                else:\n",
    "                    corr_list.append(f\"X{i}\")\n",
    "\n",
    "#    # Проверим значения, которые никак не коррелируют с 'y'\n",
    "#    for i in range(1, len(matrix_corr[\"Y\"])):\n",
    "#        if isnan(matrix_corr[\"Y\"][f\"X{i}\"]) ==  True: corr_list.append(f\"X{i}\" + str(i))\n",
    "\n",
    "    corr_list = list(set(corr_list)) \n",
    "    print (corr_list)\n",
    "    temp = df.drop (corr_list, axis = 1)\n",
    "    return (temp)\n",
    "valid_corr(df, max_corr=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[LIST_NAME_FEATURES]\n",
    "Y = df[LIST_NAME_Y]\n",
    "\n",
    "#X = pd.DataFrame(X)\n",
    "X = pd.DataFrame(Normalizer().fit(X).transform(X))\n",
    "\n",
    "# Нормализация\n",
    "X = pd.DataFrame(Normalizer().fit(X).transform(X))\n",
    "\n",
    "# Стандартизация\n",
    "#X = pd.DataFrame(StandardScaler().fit(X).transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Columns: 300 entries, 0 to 299\n",
      "dtypes: float64(300)\n",
      "memory usage: 6.9 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017599</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>-0.030423</td>\n",
       "      <td>0.019760</td>\n",
       "      <td>-0.018561</td>\n",
       "      <td>-0.002857</td>\n",
       "      <td>-0.006286</td>\n",
       "      <td>-0.031841</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013703</td>\n",
       "      <td>0.016028</td>\n",
       "      <td>-0.010474</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.005117</td>\n",
       "      <td>-0.014651</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-0.005402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004085</td>\n",
       "      <td>-0.006114</td>\n",
       "      <td>-0.013076</td>\n",
       "      <td>-0.013763</td>\n",
       "      <td>0.003931</td>\n",
       "      <td>-0.028521</td>\n",
       "      <td>-0.034540</td>\n",
       "      <td>-0.029789</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023342</td>\n",
       "      <td>0.030524</td>\n",
       "      <td>-0.005706</td>\n",
       "      <td>-0.003656</td>\n",
       "      <td>-0.022111</td>\n",
       "      <td>-0.008082</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>-0.004344</td>\n",
       "      <td>-0.011168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006779</td>\n",
       "      <td>-0.004085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006522</td>\n",
       "      <td>-0.000867</td>\n",
       "      <td>0.019998</td>\n",
       "      <td>-0.031289</td>\n",
       "      <td>-0.006056</td>\n",
       "      <td>-0.006677</td>\n",
       "      <td>-0.009578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020853</td>\n",
       "      <td>-0.035683</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>-0.012606</td>\n",
       "      <td>0.031160</td>\n",
       "      <td>-0.008356</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>0.047602</td>\n",
       "      <td>0.051126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.030423</td>\n",
       "      <td>-0.006114</td>\n",
       "      <td>-0.006522</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018611</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>-0.002702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>-0.020758</td>\n",
       "      <td>-0.001577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007194</td>\n",
       "      <td>-0.009817</td>\n",
       "      <td>0.006939</td>\n",
       "      <td>0.013916</td>\n",
       "      <td>-0.018710</td>\n",
       "      <td>-0.014764</td>\n",
       "      <td>-0.013920</td>\n",
       "      <td>-0.009440</td>\n",
       "      <td>-0.005392</td>\n",
       "      <td>0.019992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019760</td>\n",
       "      <td>-0.013076</td>\n",
       "      <td>-0.000867</td>\n",
       "      <td>0.018611</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>0.038801</td>\n",
       "      <td>0.005932</td>\n",
       "      <td>0.027716</td>\n",
       "      <td>-0.029001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018542</td>\n",
       "      <td>0.031089</td>\n",
       "      <td>-0.006569</td>\n",
       "      <td>-0.002962</td>\n",
       "      <td>0.050259</td>\n",
       "      <td>-0.007506</td>\n",
       "      <td>0.028128</td>\n",
       "      <td>0.016604</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>-0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.005117</td>\n",
       "      <td>-0.008082</td>\n",
       "      <td>0.031160</td>\n",
       "      <td>-0.014764</td>\n",
       "      <td>-0.007506</td>\n",
       "      <td>-0.014407</td>\n",
       "      <td>-0.041227</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>-0.005689</td>\n",
       "      <td>-0.012587</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017376</td>\n",
       "      <td>-0.033363</td>\n",
       "      <td>-0.010817</td>\n",
       "      <td>-0.017530</td>\n",
       "      <td>-0.017032</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026155</td>\n",
       "      <td>0.014860</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>-0.004815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.014651</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>-0.008356</td>\n",
       "      <td>-0.013920</td>\n",
       "      <td>0.028128</td>\n",
       "      <td>-0.023166</td>\n",
       "      <td>-0.006205</td>\n",
       "      <td>0.026208</td>\n",
       "      <td>-0.007522</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019738</td>\n",
       "      <td>0.009899</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>-0.031915</td>\n",
       "      <td>0.022085</td>\n",
       "      <td>0.026155</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>0.009654</td>\n",
       "      <td>-0.013598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.000317</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>-0.009440</td>\n",
       "      <td>0.016604</td>\n",
       "      <td>0.026066</td>\n",
       "      <td>0.018721</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>-0.005497</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012031</td>\n",
       "      <td>0.012170</td>\n",
       "      <td>-0.003357</td>\n",
       "      <td>-0.009033</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>0.014860</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022669</td>\n",
       "      <td>0.017218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.007413</td>\n",
       "      <td>-0.004344</td>\n",
       "      <td>0.047602</td>\n",
       "      <td>-0.005392</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>0.006157</td>\n",
       "      <td>-0.023935</td>\n",
       "      <td>-0.014070</td>\n",
       "      <td>-0.023042</td>\n",
       "      <td>-0.003614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004912</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>-0.012036</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>0.009654</td>\n",
       "      <td>-0.022669</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.035780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-0.005402</td>\n",
       "      <td>-0.011168</td>\n",
       "      <td>0.051126</td>\n",
       "      <td>0.019992</td>\n",
       "      <td>-0.001392</td>\n",
       "      <td>0.004884</td>\n",
       "      <td>-0.005839</td>\n",
       "      <td>-0.007209</td>\n",
       "      <td>-0.010200</td>\n",
       "      <td>0.024248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015623</td>\n",
       "      <td>-0.013490</td>\n",
       "      <td>-0.015048</td>\n",
       "      <td>0.013491</td>\n",
       "      <td>-0.008548</td>\n",
       "      <td>-0.004815</td>\n",
       "      <td>-0.013598</td>\n",
       "      <td>0.017218</td>\n",
       "      <td>-0.035780</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    1.000000  0.017599  0.006779 -0.030423  0.019760 -0.018561 -0.002857   \n",
       "1    0.017599  1.000000 -0.004085 -0.006114 -0.013076 -0.013763  0.003931   \n",
       "2    0.006779 -0.004085  1.000000 -0.006522 -0.000867  0.019998 -0.031289   \n",
       "3   -0.030423 -0.006114 -0.006522  1.000000  0.018611  0.017084 -0.002702   \n",
       "4    0.019760 -0.013076 -0.000867  0.018611  1.000000  0.007163  0.038801   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "295  0.005117 -0.008082  0.031160 -0.014764 -0.007506 -0.014407 -0.041227   \n",
       "296 -0.014651  0.001063 -0.008356 -0.013920  0.028128 -0.023166 -0.006205   \n",
       "297  0.000317 -0.009752  0.003938 -0.009440  0.016604  0.026066  0.018721   \n",
       "298 -0.007413 -0.004344  0.047602 -0.005392  0.008420  0.006157 -0.023935   \n",
       "299 -0.005402 -0.011168  0.051126  0.019992 -0.001392  0.004884 -0.005839   \n",
       "\n",
       "          7         8         9    ...       290       291       292  \\\n",
       "0   -0.006286 -0.031841  0.016283  ... -0.013703  0.016028 -0.010474   \n",
       "1   -0.028521 -0.034540 -0.029789  ... -0.023342  0.030524 -0.005706   \n",
       "2   -0.006056 -0.006677 -0.009578  ... -0.020853 -0.035683  0.015925   \n",
       "3    0.000543 -0.020758 -0.001577  ... -0.007194 -0.009817  0.006939   \n",
       "4    0.005932  0.027716 -0.029001  ...  0.018542  0.031089 -0.006569   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "295  0.019554 -0.005689 -0.012587  ... -0.017376 -0.033363 -0.010817   \n",
       "296  0.026208 -0.007522  0.007831  ...  0.019738  0.009899  0.003935   \n",
       "297  0.006274 -0.005497  0.012468  ... -0.012031  0.012170 -0.003357   \n",
       "298 -0.014070 -0.023042 -0.003614  ...  0.004912  0.002824  0.002700   \n",
       "299 -0.007209 -0.010200  0.024248  ... -0.015623 -0.013490 -0.015048   \n",
       "\n",
       "          293       294       295       296       297       298       299  \n",
       "0    0.001677  0.000561  0.005117 -0.014651  0.000317 -0.007413 -0.005402  \n",
       "1   -0.003656 -0.022111 -0.008082  0.001063 -0.009752 -0.004344 -0.011168  \n",
       "2    0.012895 -0.012606  0.031160 -0.008356  0.003938  0.047602  0.051126  \n",
       "3    0.013916 -0.018710 -0.014764 -0.013920 -0.009440 -0.005392  0.019992  \n",
       "4   -0.002962  0.050259 -0.007506  0.028128  0.016604  0.008420 -0.001392  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "295 -0.017530 -0.017032  1.000000  0.026155  0.014860  0.004940 -0.004815  \n",
       "296 -0.031915  0.022085  0.026155  1.000000 -0.001292  0.009654 -0.013598  \n",
       "297 -0.009033  0.005942  0.014860 -0.001292  1.000000 -0.022669  0.017218  \n",
       "298  0.003319 -0.012036  0.004940  0.009654 -0.022669  1.000000 -0.035780  \n",
       "299  0.013491 -0.008548 -0.004815 -0.013598  0.017218 -0.035780  1.000000  \n",
       "\n",
       "[300 rows x 300 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000980</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.001069</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>-0.001011</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000987</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>-0.001570</td>\n",
       "      <td>-0.000691</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>-0.001315</td>\n",
       "      <td>-0.000647</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>-0.000567</td>\n",
       "      <td>-0.001085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.058240</td>\n",
       "      <td>0.058397</td>\n",
       "      <td>0.056389</td>\n",
       "      <td>0.057126</td>\n",
       "      <td>0.058310</td>\n",
       "      <td>0.058130</td>\n",
       "      <td>0.058254</td>\n",
       "      <td>0.058736</td>\n",
       "      <td>0.058583</td>\n",
       "      <td>0.057197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074288</td>\n",
       "      <td>0.056232</td>\n",
       "      <td>0.056843</td>\n",
       "      <td>0.058198</td>\n",
       "      <td>0.057843</td>\n",
       "      <td>0.056706</td>\n",
       "      <td>0.059887</td>\n",
       "      <td>0.056979</td>\n",
       "      <td>0.058670</td>\n",
       "      <td>0.058443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.212305</td>\n",
       "      <td>-0.189858</td>\n",
       "      <td>-0.241993</td>\n",
       "      <td>-0.165972</td>\n",
       "      <td>-0.223318</td>\n",
       "      <td>-0.182358</td>\n",
       "      <td>-0.195210</td>\n",
       "      <td>-0.188125</td>\n",
       "      <td>-0.226954</td>\n",
       "      <td>-0.187129</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202729</td>\n",
       "      <td>-0.207104</td>\n",
       "      <td>-0.221628</td>\n",
       "      <td>-0.192856</td>\n",
       "      <td>-0.176561</td>\n",
       "      <td>-0.211669</td>\n",
       "      <td>-0.201630</td>\n",
       "      <td>-0.176582</td>\n",
       "      <td>-0.189029</td>\n",
       "      <td>-0.175842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.038062</td>\n",
       "      <td>-0.038565</td>\n",
       "      <td>-0.038958</td>\n",
       "      <td>-0.038102</td>\n",
       "      <td>-0.039830</td>\n",
       "      <td>-0.040371</td>\n",
       "      <td>-0.037930</td>\n",
       "      <td>-0.039471</td>\n",
       "      <td>-0.038478</td>\n",
       "      <td>-0.040436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059143</td>\n",
       "      <td>-0.036588</td>\n",
       "      <td>-0.039163</td>\n",
       "      <td>-0.040044</td>\n",
       "      <td>-0.037809</td>\n",
       "      <td>-0.039687</td>\n",
       "      <td>-0.040679</td>\n",
       "      <td>-0.039992</td>\n",
       "      <td>-0.037312</td>\n",
       "      <td>-0.041606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000709</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>-0.001123</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>-0.001351</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>-0.000340</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>-0.000317</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013350</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>-0.000597</td>\n",
       "      <td>0.001931</td>\n",
       "      <td>-0.002059</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>-0.000975</td>\n",
       "      <td>-0.000518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.039970</td>\n",
       "      <td>0.039487</td>\n",
       "      <td>0.037853</td>\n",
       "      <td>0.039080</td>\n",
       "      <td>0.038387</td>\n",
       "      <td>0.040281</td>\n",
       "      <td>0.042003</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.041950</td>\n",
       "      <td>0.037666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058046</td>\n",
       "      <td>0.038393</td>\n",
       "      <td>0.037721</td>\n",
       "      <td>0.038417</td>\n",
       "      <td>0.039873</td>\n",
       "      <td>0.037240</td>\n",
       "      <td>0.039550</td>\n",
       "      <td>0.038173</td>\n",
       "      <td>0.037639</td>\n",
       "      <td>0.039657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.202665</td>\n",
       "      <td>0.235068</td>\n",
       "      <td>0.191314</td>\n",
       "      <td>0.187242</td>\n",
       "      <td>0.206798</td>\n",
       "      <td>0.196022</td>\n",
       "      <td>0.226549</td>\n",
       "      <td>0.199384</td>\n",
       "      <td>0.192011</td>\n",
       "      <td>0.175934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273039</td>\n",
       "      <td>0.186108</td>\n",
       "      <td>0.237360</td>\n",
       "      <td>0.195017</td>\n",
       "      <td>0.199443</td>\n",
       "      <td>0.211935</td>\n",
       "      <td>0.192817</td>\n",
       "      <td>0.159957</td>\n",
       "      <td>0.180927</td>\n",
       "      <td>0.177493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  3000.000000  3000.000000  3000.000000  3000.000000  3000.000000   \n",
       "mean      0.000980    -0.000474    -0.001069     0.000339    -0.001022   \n",
       "std       0.058240     0.058397     0.056389     0.057126     0.058310   \n",
       "min      -0.212305    -0.189858    -0.241993    -0.165972    -0.223318   \n",
       "25%      -0.038062    -0.038565    -0.038958    -0.038102    -0.039830   \n",
       "50%       0.000709    -0.001708    -0.001123    -0.000339    -0.001351   \n",
       "75%       0.039970     0.039487     0.037853     0.039080     0.038387   \n",
       "max       0.202665     0.235068     0.191314     0.187242     0.206798   \n",
       "\n",
       "               5            6            7            8            9    ...  \\\n",
       "count  3000.000000  3000.000000  3000.000000  3000.000000  3000.000000  ...   \n",
       "mean     -0.000108     0.002424    -0.001011     0.001836    -0.001034  ...   \n",
       "std       0.058130     0.058254     0.058736     0.058583     0.057197  ...   \n",
       "min      -0.182358    -0.195210    -0.188125    -0.226954    -0.187129  ...   \n",
       "25%      -0.040371    -0.037930    -0.039471    -0.038478    -0.040436  ...   \n",
       "50%      -0.000330     0.001109    -0.000340     0.000368    -0.000317  ...   \n",
       "75%       0.040281     0.042003     0.037021     0.041950     0.037666  ...   \n",
       "max       0.196022     0.226549     0.199384     0.192011     0.175934  ...   \n",
       "\n",
       "               290          291          292          293          294  \\\n",
       "count  3000.000000  3000.000000  3000.000000  3000.000000  3000.000000   \n",
       "mean     -0.000987     0.000508    -0.001570    -0.000691     0.001934   \n",
       "std       0.074288     0.056232     0.056843     0.058198     0.057843   \n",
       "min      -0.202729    -0.207104    -0.221628    -0.192856    -0.176561   \n",
       "25%      -0.059143    -0.036588    -0.039163    -0.040044    -0.037809   \n",
       "50%      -0.013350    -0.000060     0.000104    -0.000597     0.001931   \n",
       "75%       0.058046     0.038393     0.037721     0.038417     0.039873   \n",
       "max       0.273039     0.186108     0.237360     0.195017     0.199443   \n",
       "\n",
       "               295          296          297          298          299  \n",
       "count  3000.000000  3000.000000  3000.000000  3000.000000  3000.000000  \n",
       "mean     -0.001315    -0.000647    -0.000408    -0.000567    -0.001085  \n",
       "std       0.056706     0.059887     0.056979     0.058670     0.058443  \n",
       "min      -0.211669    -0.201630    -0.176582    -0.189029    -0.175842  \n",
       "25%      -0.039687    -0.040679    -0.039992    -0.037312    -0.041606  \n",
       "50%      -0.002059     0.000333     0.000358    -0.000975    -0.000518  \n",
       "75%       0.037240     0.039550     0.038173     0.037639     0.039657  \n",
       "max       0.211935     0.192817     0.159957     0.180927     0.177493  \n",
       "\n",
       "[8 rows x 300 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 <-> 3000\n"
     ]
    }
   ],
   "source": [
    "print (len(X), '<->', len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(X, Y, size_, random_state_):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=size_, random_state=random_state_) \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier: 0.9683333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:645: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier: 0.9683333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier: 0.9541666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier: 0.9716666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier_default: 0.97\n",
      "RandomForestClassifier_params: 0.5275\n",
      "LogisticRegression: 0.9491666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  del sys.path[0]\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionCV: 0.95\n",
      "PassiveAggressiveClassifier: 0.9158333333333334\n",
      "RidgeClassifier: 0.9516666666666667\n",
      "RidgeClassifierCV: 0.9475\n",
      "SGDClassifier: 0.9408333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:940: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:1853: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:934: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier: 0.6358333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:404: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  SupervisedIntegerMixin.fit(self, X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0error: No neighbors found for test samples array([   0,    1,    2, ..., 1197, 1198, 1199], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.\n",
      "LinearSVC: 0.9458333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC: 0.9491666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC: 0.9466666666666667\n",
      "DecisionTreeClassifier: 0.9508333333333333\n",
      "ExtraTreeClassifier: 0.6358333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianProcessClassifier: 0.945\n",
      "GaussianNB: 0.965\n",
      "0error: Negative values in data passed to MultinomialNB (input X)\n",
      "LinearDiscriminantAnalysis: 0.9466666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuadraticDiscriminantAnalysis: 0.6683333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_list = []\n",
    "\n",
    "# Здесь будем хранить результаты\n",
    "TestModels = DataFrame()\n",
    "tmp = {}\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_data(X, Y, 0.40, 0)\n",
    "#for name, clf in zip(names, classifiers):\n",
    "for model_name in MODELS.keys():\n",
    "    try:\n",
    "        #ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        model = MODELS[model_name]\n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "\n",
    "        prediction = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(prediction, y_test)\n",
    "        print(f\"{model_name}: {accuracy}\")\n",
    "        if abs(accuracy) < 2:\n",
    "            tmp['Model'] = model_name\n",
    "            # tmp['R2_Y'] =  r2_score(y_test, prediction)\n",
    "            tmp['ACCU'] =  accuracy_score(y_test, prediction)\n",
    "            TestModels = TestModels.append([tmp])\n",
    "            # try:\n",
    "            #     fpr, tpr, thresholds = roc_curve(y_test, prediction, pos_label=1)\n",
    "            #     tmp['fpr'] = fpr\n",
    "            #     tmp['tpr'] = tpr\n",
    "            #     tmp['thresholds'] = thresholds\n",
    "            # except:\n",
    "            #     print ('error roc')\n",
    "            # Записываем данные и итоговый DataFrame\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print (f\"{name}error: {e}\")\n",
    "    \n",
    "    \n",
    "TestModels.set_index('Model', inplace=True)    \n",
    "# print_list(prediction_list)\n",
    "[print(item) for item in prediction_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier_params</th>\n",
       "      <td>0.527500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.635833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.635833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.668333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.915833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.940833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianProcessClassifier</th>\n",
       "      <td>0.945000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.945833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.947500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>0.949167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.949167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegressionCV</th>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.950833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.951667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.954167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.968333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.968333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier_default</th>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ACCU\n",
       "Model                                   \n",
       "RandomForestClassifier_params   0.527500\n",
       "ExtraTreeClassifier             0.635833\n",
       "KNeighborsClassifier            0.635833\n",
       "QuadraticDiscriminantAnalysis   0.668333\n",
       "PassiveAggressiveClassifier     0.915833\n",
       "MLPClassifier                   0.920000\n",
       "SGDClassifier                   0.940833\n",
       "GaussianProcessClassifier       0.945000\n",
       "LinearSVC                       0.945833\n",
       "LinearDiscriminantAnalysis      0.946667\n",
       "SVC                             0.946667\n",
       "RidgeClassifierCV               0.947500\n",
       "NuSVC                           0.949167\n",
       "LogisticRegression              0.949167\n",
       "LogisticRegressionCV            0.950000\n",
       "DecisionTreeClassifier          0.950833\n",
       "RidgeClassifier                 0.951667\n",
       "ExtraTreesClassifier            0.954167\n",
       "GaussianNB                      0.965000\n",
       "BaggingClassifier               0.968333\n",
       "AdaBoostClassifier              0.968333\n",
       "RandomForestClassifier_default  0.970000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestModels.sort_values(by=['ACCU']).head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 0.9716666666666667\n",
      "Лучшая модель: GradientBoostingClassifier\n"
     ]
    }
   ],
   "source": [
    "# Найти максимальное значение\n",
    "\n",
    "# ??? \n",
    "name_beast_model = ''\n",
    "for index, row in TestModels.iterrows():\n",
    "    if row['ACCU'] == TestModels['ACCU'].values.max():\n",
    "        name_beast_model = index\n",
    "#print ('R^2:', TestModels['R2_Y'].values.max())\n",
    "print ('Точность:', TestModels['ACCU'].values.max())\n",
    "print ('Лучшая модель:', name_beast_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25b772fc608>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAIaCAYAAAAEFshMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd7xcVbn/8c83AUQRUCA2IBAgNL0gglwVG3ZAQLmiIpZrgWtBEPzhRa8VQQUrKqIochFFRGyoKChNQXpvIgELEb2A0lHq8/vjWZNMJpOTk6y1T+bE7/v1mtc5s2fOs/bZs2fPM6sqIjAzMzOzxTNlSe+AmZmZ2WTmZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzGwkSTpd0q2SHjawfUtJJ0q6TdLfJZ0n6Y19j68k6XOS/iTpLkmzyv3VyuMhab2BmB+W9M2J+c/MbGnjZMrMRo6ktYFnAQHs0Lf96cCpwBnAesCqwNuAbcrjywGnAE8EXgKsBDwD+Buw5UTtv5n9a1lmSe+AmdkQrwfOAc4F3gB8t2z/JHBURBzU99wLgVf2/d10YOuIuKtsuwn4aOd7bGb/slwzZWaj6PXAt8rtxZIeK+kRwNOB48f4uxcAP+9LpMzMOudkysxGiqRnAmsBx0XEhcB1wGuAR5PXrL+M8eerLuRxM7PmnEyZ2ah5A3ByRNxS7h9Ttt0KPAQ8foy//dtCHgd4EFh2YNuywP2LvqtmZu4zZWYjRNLDyf5PUyX9tWx+GPAoYCZwNvAfwGkLCPFL4ABJK0TE3Qt4zp+AtYGr+7bNAH5Xt/dm9q/KNVNmNkpeRtYcbQw8udw2An5N9qN6D/CfkvaVtCqApE0lHVv+/mjgBuB7kjaUNEXSqpLeJ2nb8pzvAO+XtEZ5/AXA9ozdF8vMbIGcTJnZKHkDcGRE/Cki/tq7AV8EdgXOA55XbtdL+jtwOHAiQETcS3ZC/y3wC+CO8jerkSMDAfYHfgOcSTYdHgzsGhFXTMy/aGZLG0XEkt4HMzMzs0nLNVNmZmZmFZxMmZmZmVVwMmVmZmZWwcmUmZmZWQUnU2ZmZmYVltiknauttlqsvfbaS6p4MzMzs3G78MILb4mIacMeW2LJ1Nprr80FF1ywpIo3MzMzGzdJf1zQY27mMzMzM6uw0GRK0tcl3SRp6OzASp+XNEvSZZKe0n43zczMzEbTeGqm/hd4yRiPb0MuQDoT2B04rH63zMzMzCaHhSZTEfEr4O9jPGVH4BuRzgEeJenxrXbQzMzMbJS16DO1OrlKe8/ssm0+knaXdIGkC26++eYGRZuZmZktWS2SKQ3ZNnT15Ig4PCK2iIgtpk0bOrrQzMzMbFJpkUzNBtbsu78GcGODuGZmZmYjr0UydQLw+jKq72nA7RHxlwZxzczMzEbeQiftlPRt4LnAapJmAx8ClgWIiC8DJwLbArOAe4A3drWzZmZmZqNmoclUROyykMcDeEezPTIzMzObRDwDupmZmVkFJ1NmZmZmFZxMmZmZmVVYaJ8pGz1r7/fTRXr+Hz6xXUd7YmZmZiOXTHWdKCxq/MUpY7JzsmZmZjZ+I5dMmZmZ2Wjwl+vxcZ8pMzMzswpOpszMzMwqOJkyMzMzq+A+Ux1wG7OZmdm/DidTtlQatVGho5Ywe1SrtTDZ3wewdPwPtuQ5mTKzTvhDasmb7K/BZN9/G5+l4XV2MmVmZjZJLQ2JyNLAyZSZTUr+EDGzUeHRfGZmZmYVnEyZmZmZVXAyZWZmZlbByZSZmZlZBSdTZmZmZhWcTJmZmZlV8NQIZmZDeOoFMxsvJ1NmI8gf5GZmk4eb+czMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKowrmZL0EknXSJolab8hj0+XdJqkiyVdJmnb9rtqZmZmNnoWmkxJmgocCmwDbAzsImnjgae9HzguIjYDXg18qfWOmpmZmY2i8dRMbQnMiojrI+I+4Fhgx4HnBLBS+X1l4MZ2u2hmZmY2usaTTK0O3NB3f3bZ1u/DwGslzQZOBN45LJCk3SVdIOmCm2++eTF218zMzGy0jCeZ0pBtMXB/F+B/I2INYFvgaEnzxY6IwyNii4jYYtq0aYu+t2ZmZmYjZjzJ1Gxgzb77azB/M96bgeMAIuJsYHlgtRY7aGZmZjbKxpNMnQ/MlDRD0nJkB/MTBp7zJ+D5AJI2IpMpt+OZmZnZUm+hyVREPADsAZwEXE2O2rtS0v6SdihPezewm6RLgW8D/xkRg02BZmZmZkudZcbzpIg4kexY3r/tg32/XwVs1XbXzMzMzEafZ0A3MzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqLLOkd8DMzMysK2vv99NFev4fPrHdIpfhmikzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqeDkZM7MlZCKWubAly6/xvwbXTJmZmZlVcDJlZmZmVsHJlJmZmVkFJ1NmZmZmFZxMmZmZmVVwMmVmZmZWwcmUmZmZWYVxJVOSXiLpGkmzJO23gOe8UtJVkq6UdEzb3TQzMzMbTQudtFPSVOBQ4IXAbOB8SSdExFV9z5kJvBfYKiJulfSYrnbYzMzMbJSMp2ZqS2BWRFwfEfcBxwI7DjxnN+DQiLgVICJuarubZmZmZqNpPMnU6sANffdnl2391gfWl3SWpHMkvWRYIEm7S7pA0gU333zz4u2xmZmZ2QgZTzKlIdti4P4ywEzgucAuwNckPWq+P4o4PCK2iIgtpk2btqj7amZmZjZyxpNMzQbW7Lu/BnDjkOf8KCLuj4jfA9eQyZWZmZnZUm08ydT5wExJMyQtB7waOGHgOT8EtgaQtBrZ7Hd9yx01MzMzG0ULTaYi4gFgD+Ak4GrguIi4UtL+knYoTzsJ+Jukq4DTgH0j4m9d7bSZmZnZqFjo1AgAEXEicOLAtg/2/R7APuVmZmZm9i/DM6CbmZmZVXAyZWZmZlbByZSZmZlZBSdTZmZmZhWcTJmZmZlVcDJlZmZmVsHJlJmZmVkFJ1NmZmZmFZxMmZmZmVVwMmVmZmZWwcmUmZmZWQUnU2ZmZmYVnEyZmZmZVXAyZWZmZlbByZSZmZlZBSdTZmZmZhWcTJmZmZlVWGZJ74D961l7v58u8t/84RPbdbAnZmZm9VwzZWZmZlbByZSZmZlZBSdTZmZmZhWcTJmZmZlVcDJlZmZmVsHJlJmZmVkFJ1NmZmZmFZxMmZmZmVVwMmVmZmZWwcmUmZmZWQUnU2ZmZmYVnEyZmZmZVXAyZWZmZlbByZSZmZlZBSdTZmZmZhWcTJmZmZlVcDJlZmZmVsHJlJmZmVkFJ1NmZmZmFZxMmZmZmVVwMmVmZmZWwcmUmZmZWQUnU2ZmZmYVnEyZmZmZVXAyZWZmZlbByZSZmZlZBSdTZmZmZhWcTJmZmZlVcDJlZmZmVsHJlJmZmVkFJ1NmZmZmFZxMmZmZmVVwMmVmZmZWwcmUmZmZWQUnU2ZmZmYVnEyZmZmZVRhXMiXpJZKukTRL0n5jPO8VkkLSFu120czMzGx0LTSZkjQVOBTYBtgY2EXSxkOetyKwJ3Bu6500MzMzG1XjqZnaEpgVEddHxH3AscCOQ573UeBg4J8N98/MzMxspI0nmVoduKHv/uyybQ5JmwFrRsRPGu6bmZmZ2cgbTzKlIdtizoPSFOCzwLsXGkjaXdIFki64+eabx7+XZmZmZiNqPMnUbGDNvvtrADf23V8ReBJwuqQ/AE8DThjWCT0iDo+ILSJii2nTpi3+XpuZmZmNiPEkU+cDMyXNkLQc8GrghN6DEXF7RKwWEWtHxNrAOcAOEXFBJ3tsZmZmNkIWmkxFxAPAHsBJwNXAcRFxpaT9Je3Q9Q6amZmZjbJlxvOkiDgROHFg2wcX8Nzn1u+WmZmZ2eTgGdDNzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKzCuJIpSS+RdI2kWZL2G/L4PpKuknSZpFMkrdV+V83MzMxGz0KTKUlTgUOBbYCNgV0kbTzwtIuBLSJiE+B44ODWO2pmZmY2isZTM7UlMCsiro+I+4BjgR37nxARp0XEPeXuOcAabXfTzMzMbDSNJ5laHbih7/7ssm1B3gz8rGanzMzMzCaLZcbxHA3ZFkOfKL0W2AJ4zgIe3x3YHWD69Onj3EUzMzOz0TWemqnZwJp999cAbhx8kqQXAP8D7BAR9w4LFBGHR8QWEbHFtGnTFmd/zczMzEbKeJKp84GZkmZIWg54NXBC/xMkbQZ8hUykbmq/m2ZmZmajaaHJVEQ8AOwBnARcDRwXEVdK2l/SDuVpnwQeCXxX0iWSTlhAODMzM7Olynj6TBERJwInDmz7YN/vL2i8X2ZmZmaTgmdANzMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCk6mzMzMzCo4mTIzMzOr4GTKzMzMrIKTKTMzM7MKTqbMzMzMKjiZMjMzM6vgZMrMzMysgpMpMzMzswpOpszMzMwqOJkyMzMzq+BkyszMzKyCkykzMzOzCuNKpiS9RNI1kmZJ2m/I4w+T9J3y+LmS1m69o2ZmZmajaKHJlKSpwKHANsDGwC6SNh542puBWyNiPeCzwEGtd9TMzMxsFI2nZmpLYFZEXB8R9wHHAjsOPGdH4Kjy+/HA8yWp3W6amZmZjabxJFOrAzf03Z9dtg19TkQ8ANwOrNpiB83MzMxGmSJi7CdIOwMvjoi3lPuvA7aMiHf2PefK8pzZ5f515Tl/G4i1O7B7ubsBcM0i7OtqwC2L8PxF1XX8iShjssefiDIcf8mX4fhLvgzHX/JlOP6SL2NR468VEdOGPbDMOP54NrBm3/01gBsX8JzZkpYBVgb+PhgoIg4HDh/PHg+SdEFEbLE4fzsK8SeijMkefyLKcPwlX4bjL/kyHH/Jl+H4S76MlvHH08x3PjBT0gxJywGvBk4YeM4JwBvK768ATo2FVXmZmZmZLQUWWjMVEQ9I2gM4CZgKfD0irpS0P3BBRJwAHAEcLWkWWSP16i532szMzGxUjKeZj4g4EThxYNsH+37/J7Bz212bz2I1D45Q/IkoY7LHn4gyHH/Jl+H4S74Mx1/yZTj+ki+jWfyFdkA3MzMzswXzcjJmZmZmFZxMmZmZmVVwMmVmNgalNRf+TJvMyuv8+CW9HzY5jWQyJWmKpFcuBWVMlXRSR7GnSHpGF7GHlDXfWovDtlXEX1fSw8rvz5W0p6RHtYrfFUmPnYAypkr65gSU8cmOYk/6Y1SmeflhV/Gh2+uRpBdLesWQ7btKemGD+OtJ2mrI9mdJWrdB/KdK2mbI9h0kbV4bv6e8zj9pFW9QOU8/0WH8zj/TbMFGMpmKiIeAPZaCMh4E7pO0UgexHwI+3TruAgy74M53cavwPeBBSeuR02zMAI5pFVzS+pK+KulkSaf2bg1CXyrpF5LeJGnlBvHmU86haWWOt06UMjbvaD3NpeIYAedIempXwTu+Hn0EOGPI9lOA/RvE/xxw55Dt/yiP1fokcPWQ7VeVx1o6T9JTGscE5pynW3YRu8Tv7ByStLykN5QEVpL+W9JPJB0iabXGZe0sacXy+/slfb/Va9Jp7FEdzSfpA+Sb8TvA3b3tETHfzOojXsa3gacBJw+UsU+D2B8BLgO+38UkqZLeBrwdWAe4ru+hFYGzIuK1jcq5KCKeImlf4J8R8QVJF0fEZo3iXwp8GbgQeLC3PSIurIw7FXgBOa/atsDZwLeBEyLiHzWxB8r5CvAUcnLc/nPoMw3L+DQwE/juQBnfr4y7VBwjSVeRS2D9ocRXho9NWsQvZXRyPZJ02YL2c6zHFiH+FRHxpAU8dnlE/Ftl/AXGkHRpRGxaE3+wLGAj8nrX/zq3+jD/FHk9HXyfDU6EvbjxuzqHjgPuB1YAHg1cAfwYeCbw5Ih4aU38gbIui4hNJD0T+DjwKeB9EfHvIx17hJOp3w/ZHBGxziQr483DtkfEEQ1i30me3A+Sb6DeG79JTVipSXg0edLt1/fQnY0TznPJb7D/A2wfEb8f6wK9GPEvjIhmzQELKGM5srbu1cDWwCkRsWuj2B8atj0iPtIifinjyOFFxJsaljFpj5GktRYQ/48t4pcyOrkeSfodsHFZhL5/+7LAVRExszL+rIhYb1EfG5X4A/GGNktGxHXDti9G/KOHh4/XN4rf1Tl0RUQ8Sblc3OyIeFzfY60T2osjYjNJHwcuj4hjWn257jT2qCZTS5PyITI9ImYt6X1ZFJJWGevxVgmVpI2BtwJnR8S3Jc0AXhURTfoXSPowcBPwA+De3vaWCWEpZyawC/Ba4O5WNWt98VeIiLsX/szRNZmPUfk2OzMijpQ0DXhkRAz78BoppZ/OY4E9esdG0grA54FbIuK/K+N/m1xC7KsD298MvCgiXlUZ/8vA34D399fAl5r5x0fE7jXxh5T3NGD9iPiGpFWBFSLiTy3LmGx6rQeDvw+736CsnwB/Jmu0NycrCs5rkbB1GntUkylJjwD2IZOQ3ctFeIOIaNZBcILK2A74DLBcRMyQ9GTgQxHx8gaxBewKzIiIjypHHD0+Is6rjV3i/x7onSCD/Wma1uB1qcsaSEnTgVeRCcIKwLHAsRExrI/H4pbxdLIv2SMjYrqkTYH/ioi3NyxjfeAw4LHlG+gmwA4RcUCD2JP+GJWary3I68P6kp4AfDci5ut4XVFGJ9ejUptwAPAWoFeTNp08Xh+IiPsr4z+W/KJyH9mUDnmslgNeHhF/rYy/QtnXpwKXlM2bAhcAb4mIu2riD5T1fmArYN3yOq8OfCcintko/nrAocDjImLT8j7bLiI+3ih+V+fQTeT7VuR7+djeQ8ArI6LZQJPyP7yErDm6VjnC8t8i4uSRjj3CydR3yDfm68vF/eFkzcWTJ1kZFwLPB07rfQtv0Y+gxDkMeAh4XkRsJOnRwMkR0VlH2S5IeinwUWAtcomjps2VXZH0G2B1sv/DsRFxQUflnEsuIH5C3znUrBm0xDsD2Bf4SssylpZjJOkSYDPgor741f2NBsro5HokadmIuL/E6zWJzYqGfdZKOVsDveN9ZUS0GOTRH38d4Il98a9vGb+U0enrLOl04H3AoaW5ScAVEfHEsf9y3PG7OofeMNbjEXFUTfxSxkoRcceCWkRqWhK6jN0zrrX5lpB1I+JVknYBiIh/lBNvspVxf0TcNhC2VQb775Edty8GiIhb1cGIJknPHrY9In7VqIjPATuR3xa66Ei/LPA2oPd/nE4mDVXfyIH3Ar/qYp8HRcQNA+fQgwt67mJ6REScN1DGAwt68iJYWo7RfRERkgLm1Ja01tX16M+SfkSOkD299Wsh6afAt4AfRcRpLWOX+FcB3yRriH7cOv6Aewde50c0jr9CRPym97KWsmqvQ/06OYdaJEvjcAzwUjIZDOZtDQmy4/4oxgZGO5m6r2TVvZN6Xfr6u0yiMq5Wzv0xRdkXaC/gnEax71eOlurt/zSypqq1fft+X54c3nsh8LxG8W8gv5119YF7GLAs8KVy/3Vl21sq425Hjvz5cv9GSXuT1fhVfVH63KCcUyxKsrwnw4eK17ilnP+9c+kVwF8axF1ajtFxyhGDj5K0G/Am4KsL+ZtF1dX1aCOy1u6DwNGSjge+HRHnNogNuVjsq4HPSTqNHK15YkTc1yj+LiX+yZJuKfGPi4gbG8Xv931JhwIrS3oj8Gbg6w3j/618DvRe45cBVc2gAzo5h5QDVBZ0fY6IGDrQalFEGREYETNqY01k7P5CRvJGzm10BnAz+a3nD8BzJ2EZKwAHAReT7f0HkbUALWLvSg4Fnw0cCFwD7DwBr82a5MW4VbynAj8nazH26d0axr90PNsWI+5VwJQh26eQyWGr/V+tnJ//R3ak/yawauPXdB3gl8A9ZAfNM4G1fYzmKeOF5LxGnwJe2DJ2X/yur0dPIL/QnU0O/z+wYeyHk/1pfkAmCF9vfZzIaWY+C/wJOBXYrYPXYZtSxueAbRrHXq/s9z1k/7VzyD6vI30OAf8x5LZ3+R9mNz5GW5E1eJADVT5D9gEb6dgj22cKQDmS4mlkldw5EXHLZCyjS5I2JPtkiRxq3rrGYliZAi6LBv2+SryTgbuAy+mrWYt2w9ovIpPM68r9dYDjo3IEiqQrYwF9HcZ6bJSV5qspETFsEsbFibfUHaMuTdA175Fks/o+5ICV5rPUl47VRwGbRMTUDuI/l0x4No6Ih7WO3zXltDOKiNs6iN3pOVSun+8ju018Fjgi2tVCIukycoDBJsDR5OCDnSLiOaMce+Sa+SRtGBG/1dxZSXtNDdMlTY+IiyZJGZ+OiHdL+gFDqkcjYqeK2P2d6W4iq717j60S7Yf8f4G5/8MU4MnApQ2LWCUiXtQw3qB9gdMkXU9eYNYC3tgg7j2SZkbEtf0blSNoqjv3SnpPRBw8cPzniIg9G5Tx2oj4pqR9Brb3yqid9HJSHyNJZ0bEM5VzuvXHbzZIYoKuR8sD25NNZlsxtya4ehRTXxmPBV5JNsk9nhx00OJ91ov/VHL//4OscTm8lNEi9hkR8RxJtzL8dR5zmphxxN8lctqXPQe2Qxbw+cr4E3EObUTOBbgZWUP71hiYu6yRByIiJO0IHBIRRyysA/woxB65ZIr8trQ7w5dKCdr005mIMnpDR7/YINagwc50PaJRZ7oB/SOwHiCb+M5qGP+Xkl4UDYanDhMRp5QP7w3IY/TbiGjRF+WDwM8kHcC8Q8LfC7yrQfyrys9ORsAVvQ62K3YUf7Ifo9cDRERXxwc6vh5JOoacV+dX5LXjNRHxz5qYA/F3I5OcDYDvA+9peX2Q9DEySbuNvK5uFRGzW8Uveklf06VR+vTWGp3WUfyuz6Hvku/bT5HNew8CK/Ulgy2/wN8p6b1kM9yzS7/gZUc+dsu2zkZtmjuXn+tM8jJOLj8/1kHsZ5afyy/p16vR/3Mn2bz3D+COcv+OBnGfV37uNOzWaN+fRDZnXFhu3yDnLWkR++jyc68Oj/1B5Wdnfe2GHKOjJssxAi4sP0/p8Ph0ej0C3gCs2OH+Hwm8iCF94xrFPxF4dt/91wM/IicdXaXx63xyR//Dx8rPJtedJXAO/QH4fbld3/f774HrG5f1ODI5fFa5P52c6mGkY49cnynNXaet6ayqS6CMq4DdgK+R36rmnRsh4rKK2BdGxOZd7v9AeTPJJWU2JkfzARAjPmmnpI9ExIc0AUuldKGcQ9uQgwyey/znUPW3QeVaZE8Bzu3wvTAV+ERE7LvQJy967E6PkXLakR+SIz8/O/h4NFj7r+vrkaTtyT6Ofyz3P0g2lf2RTEKrZnFXLrVzW0TcXu5vDbysxP9iVPanKX0eXxARf1dO03Is8E6yu8FGEfGKmviljEvIJsO3MmTx5Khvhruc3N/zO3qNO/9MmyiS9gC+FRG3TqbYo9jM97cyvHaGpPkWf4yIHSZJGR8BPgyswdwh+XOKYO6cR4vj/pIgrCFpvjd5NOhLM+BI4EPkh8nWZJV40/m4lBOOzmTeZK1qHquI+FD52azfxjDK2cP/H7A2fe+piKhtLv4y2bdlHbJGp/ncKCX+LcAKku7o296sT0JibEMAACAASURBVFBEPKhGK7MP0fUxejWZGCxDd02hXV+PDiQ7JPcmyH0t2Sy3GXn8XlwZ/zjg5cDtyhUevkt++dqUvPbVTkEypS8pfhVweER8D/heSYJa2IWssV6GbprifgH8nXyf9Sf4TfpkMTGfaQAoZ4Vfi3mvda3mHISsPTq/JNFfB06KdrU+ncUexZqp5chvykcz5E0YEWdMhjL6yvpI70O9YczVyD4QB5F9UuYRjSdY66sJmzNzu6RfR8SzGsV/Czlcew1y+oinkbP2NpnHStJeZEJ4Jzk30FOA/aJRHy1Jl5IfShfSN1FkRFy4wD9atPiHRcTbWsQao4wfRcSOHcb/NJksf5d5V7P/fqP4nR4jSdtExM86it3p9Uh9C9FK+jpwTUQcVO5X12Sob4ZwSZ8CHoqI90iaAlwSlbOHS7oCeHJEPCDpt8DuvQ9vtV8JYPvoaGJQZQejnwDzJTYRUTXB7ER9pkk6iExor2LutS5aJmulHJFNx28k+2odR44arF5wuqvYI1czVaqEz5H0jIi4ebKWobkjmL6nHCY8uA+L3cwXOdT1WElXR0TLUXUL8s9yYby2VJP+GXhMw/h7kXNNnRMRWyune2gyLULxpog4RNKLyf1+I5lcterw/kBEHNYo1hwqozaB/9GQZRBaNPP1xeoskSpWIRer7U+Qg+ywvNi6PkYqox2BjctopsH41c18E3A9knI6hHvIaVT6a8qXH/4nixa/7/fnkYMLiIiH1GZBiW8DZygn7PwH8GsA5Tp3t7coQGW0HbCOBkbcQX0zX4kR5CS2zU3EZ1rxMnKtv9aTW88jIkLSX8n5yh4AHg0cL+kXEfGeUYw9csmUpM9FxLuAr6tM6d+vRQY8EWUA+5Gz5x465LGqZj6V4eDAWxaw/62b+d5Fjvrak1xDb2uyU2sr/4yIf0pC0sMih/hu0DB+74q+LXBkRFyqRlf54seS3k5OVjjnItMg2el8CQTNP/R/njJaNPOVQF01tXZ9jHrLxjyyMs4CTcD16HNkje8dwNVR1keUtBltZrk/VdJxJdajyUkpUS4iWz3/UEQcKOkUcrqFk/uaZaaQfadaeHT52cloPnU/9cJEfKZBdj5flvYrhcxRktk3kN0PvgbsG7m25BTgWmCxE55OY49gM9/mEXGhpKGTaDVq5uu8jC71qqK1gPkxWjXzSTo6Il4naa+IOKRFzAWU8wOytuhd5DfbW4FlI2LbRvGPJBfbnUH245hKrlG2eaP4wzrwRox4B/2JVPqVHQY8NnIB1k2AHSLigCW8a0vcBF3z1iTP/zMj4qGy7fHk++xPlbFFNv08nlzm5c9l+2bAYyLipKqdXwpImlJq6oZOYNqgmW9CPtMkfY+8hp7CvF8cm32Bl7Q/2ez2xyGPbRQVE1N3GnvUkqlhlJ2T16xpGltSZUjaCfhFRNwpaT+yXfvA1s1zJbN+ZGnyaBWz89FkQ8p8DrAy8PNoNKtuOTZPJofw3laag9bo8nxqSdJWZN+TuyW9ljyHPlf7IThQxrrkshD3KmeX3gT4RjSaoVnSGeTkqV+JiM3Ktmb9Xbo+RpIOBg4gm5l+Tn6gvKs0ATbXxfVIpe9jq3hLI0kfJzvP3wP8lLxu7B0RxzSKvzZwY0TcJ+mZ5Pvsmy2v231ldXEOdfYFflgz/UAZi/1502Xs/iAjeQNOB1Yi+1r8iazG/8wkLOOy8vMZwFnkkORzGsU+puz/CsBvyWr2fRvue2+x2HvpaG4RGq/RtoAyhq3HtFbD+MuWY3V8ue1BfuNvdg6Rieym5fe9gDMaH6NLyGb/9cg12z5LLlbbKv755efF/WVOlmPU21dy1NpR5ZpRvb7jQBmdXo/ILgdPbbnPA/HvJJsS7wD+SXZQrp4vbiJvfa/zy8jO3NNavs7lfbYssG65pn4B+MlkOYc6Pva9OawGP2uqP2+6jN27TWF0rRyZre9E9nPZnBzBNtnK6FXfvhT4UuSQ3lZrSW1c9v9l5MR204HXNYpNRHw+IjYCvh4R60TEjL5bkyasyCaHSyVNbxFvAQ4jlzXZlGwT/yM5uWbL+JuTHXu/VH5v2SH9gcgrQm8JhENoP0z/ocilIV5O1ujsTTbbtHJLqf0KAEmvoE1/nZ6uj1FvluRtyRUAmtfK0v31aGvgbEnXSbpM0uXKtcqaiIgVI2Klclue/OLYxQoQXer1I+69zjczZJmiCg9FxP3ka/y5iHgn2QWhlU7PIUkzJR0v6SpJ1/duLWL3PleGfNZUf950Gbtn5Dqg91mmtOm/klwPaLKW8RdJhwIvAbZQDmFtlcQuK2lZMpn6YmRHuubtthHxtlIlPTMijlROzbBiVE721+fxwJWSzmPeYfOtOk12udYT5Lf9Tfvun6qcLqGVLpdX6Llf0i5k58zty7aWZbyDXEttQ0l/Jr8RvrZh/K6P0Y+Vw/L/Abxd0jSy9qWlrq9H23QQc4Ei4oela8Nk8jPlVAwPAu8o17qWna0fkLQz+aX3ZWVby/O063Oo8zkHoZt5BzuPvaSq9MZRLbczWV3/pXJ/HeB7k7CMR5In9obl/hOAbRrF3pOcpuBEmLOA7687eC0+BPwY+F3f/3BWw/jPGXZrGP8Mcrj2teSkbVOByxvGvwhYt+/+OsBFDeN3tgRCXxkbk8tz7FLuzyDn4mp9Lq1AB0ubTNAxejQwtfz+COBxjeN3fj0qcR9Tjs90YHrDuP3LNb0C+AQ5X1zT/e/6Vo7PMuX3FYDVG8Z+Ell7/dpyfwbwP5PlHGLusjuX921r+plDzpN1OTkQ6TTyC8ypox57UnRAn8xKh8O/RHbs7bTDYSlvmWi8krdyluHNyASh13l4zkR9o07S44DXkP12fl2aFJ8bEU2a+iQ9n/zGdj1zk9o3RsRpjeKvQE4f8WAZFbch8LPI5oLmWnZcVZmnSdI+wx6PBvM0lXI6PUalNuHnkQNJ3k92cD8gIi5qEX8iSNqBXAj3CcBN5Hl6dUQ8sVH8I/vuPkCu5/bViLipRfyJsIABQx+LiFYzrfeXtTKZqF210CePCElnAc8i+4aeSn6Z/0RENJvKRrn0Tm/ewSerzDsYEa8a5dgj22dK0sGSVpK0rKRTJN1SRulMqjLIdb0eKv1FvgFsRHYcryZpr7L/knSEcor8JrOGD7gvMuvu9XdZYSHPXySSnibpfEl3SbpP0oOad2mTKhHxV6C/r9ot5JxQreKfQlYb71luG7RKpIpfAQ9TLuNwClm1/r8N4yPp9HIurQJcChwpqUWi84jyc8UF3Frp+hh9oHzAPpNcfuUo2vaLm4jr0UfJ1QV+FxEzyAk8z2oVPCLe2HfbLSIOnEyJVPHh8jo/g2zu/g65ukET5XVdqXxhuRw4RtJ8awFWxO/6HOqfc3BzsrmyZZcJKPMOAnPmHQRaJWudxR7ZZAp4Uam9eSkwG1ifHFo92crossPhm8r+v4gcdfJGsmq9teMkfQV4lKTdgF+Sy7K08kVybaxrgYeTVbHNOq6WfT4e+ErZtDqZ5NbGfV75uRM5s/F65Cid7cq2VhQR95Dn0Bci4uVAk9qEPl11XF23/LwqIj4yeGsQv6frY9QbSLIdcFhE/AhYrmF86P56dH9E/A2Yopz36DRy6H8VSR8c4/aB+t2eUF0OGAJYpe99dlREPJn6tRH7dXoORcT5EXFXRMwuSfNOEXFOq/jFbEmPIq/Rv5D0I+DGUY89yh3Q5xs9o6aTVk9YGV12OOx6Zm8AIuJTkl5IDnneAPhgRPyicRmzJE2NnLzuSEm/aRj+HcCWwLmlrGsltVgO5zlkVff2Qx4LKpdK6SNJTwd2JWfVh+z31VJXHVe3Lc1i7yXX5etK18foz+ULxQuAgyQ9jPZfRru+Ht2mXFbm18C3JN1ENsfVunvIthXI12FVskZssuhywBDk+2wa2bdpvnVVG+jkHFKZYV3SjxkyujEars1XvggBfFi5ePPK5NxuIx17lJOpiRg9MxFlvAl4O3BwRFwvaQa51lQLF0o6mezE+F5JKwIPNYo9j5I8NU2g+txTLlqXKCdH/Atzl/Fo4d7ISfKA7FdGg+HOURawju6WSunZi0xGfhARV0pah+w82dL+wEnkDNnnlzKubRD352Sz6goDTbe9ZTSaLFdD98foleQH7KciJ359PO1rsbu+Hu1YYr+LTDpXJl/3KhHx6d7v5Rq0F1lLfizZR2syeSWZiHwhIm6V9ARyabBWDiQHxJwZEeeV87TVqGjo7hw6uvz8VINYC6UcjftY5h6bx5HzZo1s7JHugF7ale8onUofAaxU+r9MqjK6ovln9l6V7NDYZO4YzV2vbahWH4SS1gL+j2w22Zu8yH8pImY1in8wcBvwenItr7eTzU5NamAk7UV2QL+TbP58CjkSrtVCypOepB9F94spd67UaPYPqW42C32J3+n1qLzXZkbEL0v8qRFxZ4O4q5CjKXcl+5MdEhG31sZdUsr/0/86t2pm6txEfaapu1VD3kmOIP8/5lYORIsBT53GHvFk6knkkO3+k7rlZIudl6HseH7gkDLWbxS/s/k4+srYn1xh+2iyRmFXcnj7wQ3LWI4cgRXANdFoKZkSewrZ5PAicv9PAr4WjU5+SZdGxKaSXkw2KX6AbHZ9SqP408jJRp/IvK9zs8EGkpYnj9FgGW9qVUaXuj5Gmn8k3HTgt9FoJFxfOZ1dj0rfwd3JfjvrSpoJfDkinl8Z95NkH6DDgUMj4q76vV0yJG1HzqG0BvA38vW+NiI2bBT/YcB/Mv95unuL+KWMLs+h04EdyFatS4CbyZUGho7WXcwyZgH/Xvr3NdVl7GZzQ7S+kdnjaWQGeST5YX78JCzj12QHw8vJzrgHAPs3it3ZnBkD5Zw7nm0V8bcDbiCXQjiDrHJtNRfXVHIqik7O01JGb8mgQ4CXl98vbhj/ZDLRuZrsp/V14KDG/8N3yb4t15Gjc04maxdq455ZfvaWGrmz79ZsqZGujxE5wnHV3utKTlh4eOPXoNPrEfnhtxzzLulTPd8a+Q3/H8y7nEzvtZ50y8mQg3l6r/MLyYSzVfzvkGv/XV/O11OAz0+ic6h3XN5CTikw5/rXsIzTKPN8dfD6dhe7i6CN/unLyY5/l5b7jwV+PAnL6GySs7L/yzN3PakNge908Fr8hqyNmlqO167AbxrG/y2wXt/9dclv/a3inwQs1/q49MU/snyYX0sOG16x97o3Pocu69vWem2+i/vLIDuyNk/MO3wNOj1GwAXl56XAlPL7eY3/h06vR5QvQH2v9TKtPwgn+23gde613DR7nbt+n03AOXQ5uWLFyZR1HjtIpo4AziT7QO7Tu4167FHugP6PiHhI0gOSViKr1pusoTPBZdxbRthdJ+mt5CRnLUaSQZkzQ9KcOTMkNZs8rc9ryFqXQ8hmuLPKtlZuinn7R11Pvhat/AE4S9IJzLtcTZMJI8lvmL2+a/eU/hYtO6X3Jp78S2mGuJFshmipV8ZtpZngr8DarYKX5u7ZkZPXPpecvPYbEXFboyK6Pka9kXC/ou1IuH5dX4/OkPQ+4OFldO7byZUNbK7blfPonQl8o7zOLQf19L/PNiJrkNZqGL/rc6irgSr9/lRuyzF3+pFW/ZE6iz3KydQFyvkgvkqufH0XcN4kLGNvckmZPcm+UyuTI/xaGJwz41bazccxR0T8gRwJNJSk90bExyuKuFLSicBx5Im9M3C+ylxNEVE7xcCN5TaF9gsEAzydrB28WzlB3lPIxLOVA5SzJb+bXGV+JfK8aunw0v/uA8AJ5Dnbcuj298ih5uuR3w5PICev3bZR/K6P0Y7kqKi9aTgSbkDX16P9yMT/cuC/yGWovtYw/tLgZeRafO8iB6yszPCpTxbXEeV99iEyKXlE+b2VTs+hiPgufVOcRMT15ILWzcTA/HOlP2eT16DL2CPdAb1HuSTLStF41MBEl9ElSc+hzJkRDTtvj7Psi6Kis7XmXYZiUMSId4KWdBmwKVnbcjSZLOwUEc9Zojs2QnrniKR9yRrVL0i6OMryRDavyX49siWvi3OojIw+gOwj93PyuveuiPhmqzJKOVPJAUO7lJ9nRsQrRjn2yCVTksb8UI4Ga2FNUBk/YOxpBRZ7huzSjLRAEfH3xY29OLr+UKyt+dLwieZuBy4AvhJleYGK+L1E4YPAnyPiiNoEs8T9AmOfQ3vWxC9ljDkKp1VTqKRzgc+RE4JuHxG/l3RFRDypMm6nx2iM6UGazZM1EdejUs5WwIfJZqVlmPs/tO7aMOmUWv2xXucxr7njiD/meRgRn6+MP1Hn0CWRa9q9nKzF2xs4LSI2bRT/2WQXku3IGrWtgHUiVzcY2dgwms18Y03yFrRZe24iymi2HMoQF5L72T+1be9+0L7f18J0nZHvTI6AWVzXkyN0epOlvorsq7A+WR3+uqq9gzslvbfEeVb55tNilvsLGsRYmC6aPYd5I/BW4MCSSM0AWnyb7fQYRcREHJ+JuB5B1pjuTV4/HlzIc//VrNZx/Gkdx5+oc6izWfolzSb7Mx0G7Bu5RuLvGyVSncWeU8ao1UwtLSStBqwaEdcMbN8QuDm6mOdiCZmAmqmq+JJ+FRHPHrZN0pVROVeQpMeR33jOj4hfS5oOPDcq53Yp7fkrRsTNA9sfQw45bz1b/4RQw8n+uj5Gkp4KrBYRPxvYvj1wY0RcWBN/Ikk6NyL+fUnvxyiStDn5Op80sH078nW+eMns2WiR9AmyRuof5BJdjwJ+0uK8knRIiX052Z/yR+Qo+OrKgS5j94zcQseSXitpvpoCSbtJajKCbCLKAD5PTvg2aAaVnZMlvVjSfG28kl5TRuk0I2mqpIV15O1yzTWor/maVhIcAMrvvW+i1f3LImcX7l8Q9RbgB7VxyXPoWUO2v5CcWLCacpX5tw7Zvrekg1qUUeKdrlzNfhVy2PmRklo0IXZ9jD5Jzl016OryWLUJuh4BnCbpk5KeLukpvVvD+JPZJxk+Ku1aGiyhIunjkv5ryPa9JR3YIP6EnEMRsR854GaLiLifHB3dZGWDiNiLHEH8GXIet9+R1+5XKkfSjmTs/kJG6gZcTH7THNy+Eo3m7pmgMq4c47ErKmOfA0wbsv1xwNkdvCanL+lzovLvtyWreE8jJwb9I9luvgLZebJ2/3YDzgeuK/dnAqc0iHvVGI8t8Pxa1DIo8yYNbJ9Se54Oew1pPNlf18eIMSa1pMzl0+LYdH09KvFOG3KbNHOJdXlbyOvc5DxdwPtsaov32QSeQ68fduvoNVmWHGl3DHDLqMcexT5TQ9eKiog7JLXohzJRZYx1bGvLeEQMNGtA1pAo50hp7SxJXyRn7+2fp6lFR/2pwJ4RMVYtQlXNV0ScqFw6Y0OyX9lvY27zz+dqYhfvIKu8zy3lXVuamWqN1RmhVa1yRMR88+hEzlXTpjNEWka5OPAryU7orXR9jB4+xmOt3msTcT0iIrZuFWspNNbr/IgG8R9awPvswUbvswk5h4Cn9v2+PPB84CKg5RJse0XEIZE1Xz8mF29+36jHHrlmPmDZYQmBcjXy5YY8f1TLuE65VttgGS+ifpXw5SXNl6yVN81YF4XF9QxyLan9yY6On6bR6uER8SALqSaOiI/VlKFc7HNfYI+IuARYU9JLa2IOuDf6pqMor02Lzog3SdpycGPpxzNfMr2Y7imJ5mAZM8l+Ea30JvubFW0n++v6GP1S0oGDH3iSPgKc2iA+dHw9Us59hqR9ht1q4y8lTi2v6TyUI3RPbxD/n8qJawfjr0vOX1ZrIj7TiIh39t12AzZrGb94w5BtO4967FGsmToCOF7S2yIni+zNl3FoeWyylLEPmfWeQY6eAdgCeDb1k4R9H/iqpD0i4m6A8kb6fHmsqQn4RttZzVdxJPkaPL3cn03Wdv2kUfwz1M3M0vsCx0n6X+Y9h14PvLpBfMiJOX8m6YCBMt5LTlzYRHQ32V/Xx+jd5DVhlqRLyrZNyVGEb2kQH7q/HvU+ZIeNTPQIpPRu4OuSfkc2mUGuanA5bVYz+BBwoqSPMu95+j/A/2sQfyI+04a5h+zWUE3SLuRAnhnK1Sp6ViQXnR7J2HPKKO2HI6V0iH0vOQtzkB+wn4iIwzoqA3Km2NZlLE8Ol+/NpXMlcHREVH3jLzUfB5AX8z+STR1rkm+aD5QqzGYkPRb4GPCEiNhG0sbA0yOiyZtU0mlDNkdENBnOK+mCiNhCfaMCJV0a7eZGmULOLP0i8rU4CfhaNHhzlebCdzDvOfTFiGi23I5y+Zh9+8q4AvhURFzesIzlyWP0ROZdzb56QtZyfr6dbo/ROuS+Q/bFur5V7BK/82veAsp9V0S0aOpeKkhan3lf5981jL0p8B7mfZ99stSWt4g/EZ+b/XP2TQE2Bo6L7JheG3stcoDWx8nZ+nvuJPutLfbyTV3GnlPGKCZTMOfidRO5j3eWbTMioraJbLCcR/aX0Zqkh5PNQA+VKt0NgJObvHgZe71yd1ZtkjZGOT8ja3f+JyI2LcncxRHxb12U15qk35Bt+2dFTq65LjlHynzNQ4sReypwVES8tjbWQsp5ODA9Bqba6KCcR0bEXR3E/S65oPVryCa/XYGrI0fZtCpjGsCw/oQVMR8DvI98n10OfDwi7mgVf0h5nV6PhpT3p4iYvvBn/usotRjrRMSBktYEHhONp8CQtHx0NLVJl+eQcqWNngeAP0bE7NblTEaj2Geq5/iIuGvghDi+ZQGlv8DuwG59fQjeLOnJDYv5NdnH6fHAGcDbgK83ir0NWcU6E9hG0k6Snq82nZ/7rRYRx1EW/CyJYLNJ/yQ9VtIRJWlD0saS3twqPlnF/nOyr9S3gFPIb4jVSp+vaZJa9xuYQzmn0SXk/4CkJw9UVbco4+mSriJHHSFpU0lfaljEehHxAeDuiDiKHE1ZnYwrfVjSzWSydo2km0tflxa+QX7D/wL5jb9qpuqxSNqLvCbfJelrki5S9rHsUstBBpNe6W7wXKD35ehu4MsN428p6XJyaH7vffaFhvE7PYci4oy+21ldJFLlc+xaSbdLukPSnZKafIHpMnbz4Yy1N3LE1X8A1wE79d3+k0bDwfvKOoY8qXudqn9Lrq12PvCeRmVcVH7uAexXfr+kUeyfAn8n5zj6Htn2+1OyY+/rGh6n04FV+/6XpwFnNIz/M3KU16Xl/jKMMVR5MctYlfwAfymZHLaM/ZVyznyA7Cu3D7BPw/gXkusuXty3rXq49kAZ55JNxf1ltJwa4bzy81dkM8dqwPUN4u4N/AKY0bdtHbKpde8G8S8ZuH9Ry+M+ELt3/r+YXAh60y7LK2X9qcv4k+3Wd43rfx80mQKjxDqHXM6nq/dZp+dQufafT3aLuY/8Un1H49dgFrBRR69vZ7FHsQP6BuQH3qOYt6P2neR8Pi2tCjwlSrOGpA+RtV/PJj/ADm5QxhTlyKLXkLVgkHOLtPAQeWL8H8zpO3IY8O/kh9bRjcrZh3xjrivpLHJphCaLTharRcRxyiVZiIgHJDVd7iJyxvmfAkjaQNLHI0ejtHBjuU2hm+VZHoiI29V0poL5RcQNA2W0fA0OV858/gHyXHok2fm91uuBF0bELb0NEXG9cgTbydRP3Kmy370DM7X/frRdB7NXxrbAkRFxqRq86Bp7fcEuRv9OZveXPpABIGlVSo18I1Mi4o8dvs86OYf6fJEc2PFd5g70WG/Mv1h0/xcRwybKHenYI5dMRcSPgB9JenpEnN1xcdOZdwbs+4G1IuIfku5tVMY+wEeAn0bEFaUv2K8bxV67l0gVNwHrR66X1KwTekRcVNrKNyDfrNdE207ud5eLVu8C9jRyIeIqkjYhp3B4AvBDsqnmS2SyOdZaVoskIuYbUt3YFcpZjKcqpyzYE/hN4zJukPQMIEqT5Z4Mn/l7sUTE18qvZ9B27chl+xOpvvJuVpv5dVYmv1j1fyD1Rpm2XgfzQkknkx1l36sc1l79QR4Ts77g0uJQspZ/mnKqhFeS1+9WblBO5RGlv+U7KU1+jXRyDvWLiFmSpkZ2cTiy9Elt6QJJ3yGv2XM+hyOixUj1zmKPXDLV5+WSriTnuvk5WV35rohosThqzzHAOZJ+VO5vD3xbOc3AVS0KiIhTyTlMHlbuX0+OPGrh15J+wtwh5/8B/Krs/221wSU9LyJOlbTTwEPrS2p1ckN3NV9fJWvqzgZeQn4IHgPsGg07f2reES49t5PD57/SoKx3kkOo7yX3/yRyNGdLbyWXOVqdnDriZHIUYRUtZB6jiKhdUmas5YBaLBW09oIek7R6bfy+WCJr6qaRzZ/3lC8YLYbl2zhFxDckXQi8gEygd46IKxoW8Tay3910crH1X5ZtrbyZnNKhdw6tQttz6J7yZetSSQcDf6Hd5LU9K5FTLvT39QraTPvTWexRHs13SUQ8WdLLyQUK9wZOi0bD2fvK2QLYinzjnBkRTVehL99CjgBWjojpyuGxb4mIdzaILTKBmrP/wPei0Ysq6cMR8WFJRw55OKLBsPa+spahcc1X7xzqu38DWZvXtAlRuYjmNODbZdOrgL+STSgrRcR8a2YtYvzNYpIutFqazheotlavNAffPewhYPmIaDn782DZTUfCSbowIjZvFc8WT6kZX78kVqsCK0TEn5b0fo2HpK3Ifn53l6bupwCHRMQfG8Vfi0wClyM/k1cCDouIWS3iT2ajXDPVuwhuSw5j/3tHfUYuJvu7LAMgaXrjN87nyT5gPwQobdhNJsEsSdPxNB7l2OfW8vOIiDizdfAJqPlaXtJmzG2iuQvYpNeHINpNCrpZRDy77/6PJf0qIp5daldrfUY5GvS7wLER0SLmPCQNG6V2O3BBaXpfLF03gUZEq/6Hi6P1BekcSU+NiPMbx7VxkvR+8svpuuRIzuXJ2uBnNoq/HtmU+LjIaWY2AbaLiI+3iE/WxG+quXNaHUH+H88Z868WQtKOwBoRcWi5JVSn0AAAIABJREFUfwbwGLJW52yyY3cT6nZOus5ij/LUCD+W9Fuyk9spyjlkms7LIemdZJb9C3I27J/SblbsnilDvhU0qRnpdJhn6lUPdzUcvJeAbD/k1mK5l7+Qq4T3Rmv+te9+k+VwimmS5tRQlN9XK3dbNDVtTQ7XvpnsyH15uei3tDzZPHBtuW0CrAK8WdJiT+oo6WDlZIKD2/eWdNDixh2jvNUlTS+3rr8stq7W35pMqK6TdFl5nS9rXIaN7RXkF/i7ASLiz2TtSytfI/tg9foxXc7caRhaeKB8yd6RrJE6hDaDYt5DdsXoeRiwOXldatlMCTlw6nHkiMQzgDXIAWgjHXtka6YiYr9ysb0jcjHIu1nIGm6LYS9ggzLSqytddjg8GNi+w5EPV0v6A5ks9F/URVaMbVIZv9Oar5i4hV3fDZwp6Try2MwA3l76rh3VooCI+CvweeVs8e8h+9e07De1HvC8KJPJSjqM7Df1QvKCv7heytwZn/sdAlwG/HdFbJQjQJeNiP3LprPJGrVlyWNf9Y1fOQfQgkbCPaom9hDbNI5ni+7eiAhJvcEwLRY57rdCRPym18pSymo5mOfO8p54HfCs8pnToql7uYi4oe/+mWUk6981ZE3ASutFxM6SdoyIoyT1+omOdOyRTabKSJzXAc8uJ94ZNJw8rbiBBqPGFqLLDoddDiElInaR9DjyZNuhgyLeSH6ofp5s2++EpHcA34qI28r9RwO7RESTSSkj4kTlKLsNyQ/Z3/Z1Oq9eqkPSRmQ/rFeQc4kdSyZwLa1OdiTtvR9WIJcPelB1I1sjIuYbTRS5IkCLZrKdgWf13f9bRGxWPkTOoDKZIgcRLM5ji6wMmd+Uuf/PryPi0pZl2EJ9X9KhwMqS3kg2CbWaZBngb5JmMHfk8svIGvNWXkVOw/OmiPhrqSX/ZIO4j+6/ExF79N2d1iB+v15yeZtyqau/AmuPeuyRTabItt9lyaHskInVYbRbXBTgeuB0ST9l3mGStSOM5ohcH6zVorSDuhxC2ov1V3IkZRe6rvnq2a3X1k8GvlXSbsw9t6qUb6/7kNNq7CZppqQNIqJVk/GRZOf2F0XEjY1iDjoYuETS6eTxfzbwsfKt85cVce+RNDMiru3fWJLPJssfRVnsuzikbHtQuQRPbewmNYvjoZy9ejfmjiz6pqTDI6LZDNk2tog4SNI2ZPP8psCBEfGzhkXsQfZj2lDSH8muCM0+H0oC9T3mLj58C/CDBqHPlbRbRHy1f6Ok/wLOaxC/X1dz0nUae5RH8823EO2wbZVlDB1p1LLTbJcdDtXxKDtJx0XEK5XLH/SfKM2SnbFqvhqOQLkM2LQ3yrHUWlwWEU8c+y/HHf875FxEr4+IJ5UP8bP7RxJOBqWT+5bk63tei8StfDB9gWyS7K1vtgW5IOu7IuLEyvi/A544OPpTORXJFRFRtaK9FrJsT0Q0q7Et5+nTe8lhSWTPbvilwsZQrgsnRsSLO4z/soj4nqSVyc/f6ilsBsrYjZwcepWIWLd8aflyRDy/Mu5jmPulvTdwZ3Oy79TLYt75Dv8ljXIydRE5x8d15f465Hp9nTUHdaF8038fcGhpfhB5kW/yQd4lSY+PiL8oh8POp1Wy0zVJnySrcr9MJoVvBf5/e+cdLllVpe/3a7JAEwQMCCKYYJQoiMBIUPwJjghmBBVEhBEDggHTKBhwUBgVHRXJCDgyJmAAidKCEmxooug4NCCjDqAISA7f74+1T3d1ce/tcPapW9W93uepp+qcc2vtc++tOmfttdf61h9sV1kqk/Rr2y+RdLXtjcq+1o7/gJzZCb9PNSoeSzj9I8zOnboe+IrtNrlYje0vEgml77P9QNm3LDGB+ZPtj7e0fyeRDnAq0XJnjqVJ2xe3sd831nXAps0SsaLy6EqPSEPxhQGFZtxu7qiZtaRf2P7Huf/kAtufQUyILu+5Fl1X6zMkaTuiEg6ivduFNez2jbEUIfmzFj2rZz15kUNpe5iX+T4CXCTpZuIC9mwqiY9J+qrt/TW22GLV2SYdJBxK+qjtw8ZLjrX9gTb2e+z8qby8C3iw5Lk8n8gNah36HoSzUPgYsA+RqyYisfroCd8xfzxSolFN5GsdepZdW/DB8lyjsnE8GiX4pYmI0TXE32h9wnloXRLuED18J4Ck5VzaN1Xi08AXgNvKsglEfuIx5Vhbnk4k4e9K5KL8FyHVUl2egljOvVzSj4n/weuI3yMZHH8nBCnPpUe/zPaE4rPzwc8k7Q/8R5/9Ws7bw7Yfae43iorWahGT4jxVd6D6+CmRuzmdOtfRgdgeWmfK9gUlRNkIOd5ku9Yv3/Ssq1kePx5dJBw2SedVE2AnYBpRGbIScEEZ9y3Abi3tDsJZoCRAf6s8uuAzhEr/GpJOJnRq9mhrtGtntoyxLYCk7wPvaaJFJZr04RpjFHsvIxyD5YBGvHYf2227AWxM5EkdTFQkbkNIazyFKGlv1TvPIfB6DnBOmdXuSuRZHlI7l8n2ESWS3Tiwe3pExVpHmPNplyM4N/YpzwcS9wSV51rirxdL+gSwjKTtiW4bZ1SyPSieZfvVo2Z7mJf5liAiCY0W0c+J1hw1y0g7p+RMHUV0276TknBo+5YKtt9k+7S57aswzlW2N1boci1TomKzlrQq2F+WMZyFtv/rCSJfANTMRVEoJW9OXBwv8xj94lrYnk5UeK1EdJ3/NfCA7bbObO8Yc6jFj7evhf3LiWrE03uWH663PZZswvzYvQp4pUPU9+VEpeP7Cc2sdW23bktUnKjXEI7UWkTi6rEODaKqlGXXfyR0iC6tscyazB1Jx9veo0P7m9u+rCv7PeNMISoQX0Vci34GHO1hvdGPgaSjgCNrpAEM1Paw/o0lHc1srRiIar7HbVer5hvnJtv0VPu8W+pPdZ1w2Dg5c9tXYZyriRnOvwF72b6h8jp8J87CZOV8SXoB8GHbe1ey16kzW8Y4lVh2+B7xndgdWM72rpXsX277pR3klc2yoShpv9P2Z8t2a2dQ0glErtfZhPp8zT5t/WP9CyH18EPiRrgzcJrt2n0Ykz66uG4O0n7POMsCD5WIanMPWqrJJxwFJN1IRJlnEktxNXNEO7M9tMt8RCJm74X2Qkm1NVfOJtTITynbbyX+uPcAxxPLBQuMozx7f6JfXjU9K0WF1I7A6pqzDchU4LFa4/SwP1F99ePiSK0NXFTRvhxNOfciZg2HFQeuFT3LZO+1PYc4pEIQtq1g5PrEUvEziUqXIwm5hZcyOxepBirLZLsRs06o/93dk4gEN0uv06i7LPoHSVsQ4rVLAh9g9nJ1GxaTtLhDbPQVRCVTQ42/0dsJJ/P5wAc0WxqruQjXVMfelWhN1CSgf4monEpnqnueojlbT83BCEUILyCaNDd5icsQOaJbTNoZzT9ditd2ZnuYnanHJa3jOav5qjaoBba0vWXP9nWSLrW9paJJZA26SDj8IxG92YnZ5eYQsvgfamF3TErF0sUwK4x8V60k90LXzsL2PNlx2mGMffPLdwmH41fAq4kb3ylENVDN1kddO7OU8/238uiCfYncptWB24kL/H4V7J5K5IncRehW/QJmLa+3nsDYHmTLrVuIQoDms7MU8D8DHH9RZnViAjSWM2Vgu5b219YEMhsVi56W7i3wsP131Vdx7wRJU8t9sVbrmIHYnjXGEC/zvYKobpmjms92tZtIiXS9x/blZXsz4LsOPagqyyiSeiX4ZyUcukK3eUlLNHlFJTl8DdvVe3kpJPf3JZzZ6cAKwBG2ayjrImlrIiHzUodo3tqEBlErh03SPxPLk+swZyPO5ctYrRzm/mWk8r9eqwmxd0FxZperVf0zyLyyrpC0OfAM4FzP1mh6PvF3GvqIgmZX5a4JbEr0CjUxCbjEdleiv0mh9rL5GPb/mwkEp11JYkPSpcD7m8+9pE2Ab9h+WQ37XSLpTNv/JGkms++VDba99jDanjXGsDpTMCvxs4tqvsb+pkSrgOXKGPcSH/gbCGHNH7Sw3XnCYan82YmI4swgEtwvdr0y3macGbY3lLQbIdT2MWB6Fzfams5CyVNbiWgpclDPofscfaXa2r+JWJppvpgnE+XzgnpLA106s4PKK+tbjm64B/i17Z/WGGNUkfTOiY57gCrsiyoDcKYGlTO1KVGE0QjuPgN4i+3p478rqcFQOlPlwn6/7bvKrHMr4Pe2f9LReANJDq9NcwGQ9G4iKvUZSdfWdnIk3UBUR51CzHIurpE83GO/68jXOsDtth+WtA2hoXRi2/+3ounweNh226WBZpyBObM9Yy5GVJ2eXMneUUSVZlNp+gZi0rIGcLPt/WuMkyQLgqRX2T63Q/s/sv36ruz3jbUEcwYhRqoCHmblo67FnMKaVdqkdWV76HKmJH2a0OixQvvmlYQswmskbVPzoivpgL5tKIJetmfUGqdDFle0AHkz8MkOx/kOkc9xDTCtOLs1FYLXs31vcRbOojgL1GnQCVEd9ZKSR3MMUdp+CpHEv8C4aDQNgCXKBXJnwpl9VKWrfVskTSVyl1Yn/i7nEf3DPkxEO6s4U0QFzXYlURxJ3yLyprYHqpcpjyKStgQ+S6Q0LM7slIDWSxDJxDSOVFf/g15HSqHhth6RH9ccP7GN/R7bT5IUkjRSkkKSjiUmvDcQEiEQS3OtHZ4ubQ+dM0Usm6xLiO7dRvS0e0Ch5FrbwXlJeTSiZq8BrgT2lXSa7cNa2B5EwuEhhI7IpbavLLlG/z2X98w3tr8O9C7T3CqppiPRmbNQeML2Y5JeD3zV9pE1qgUbJO0HnNxEukr+2q62qzRSpltn9iTgbiKJ/t1E54ElgddVnlCsDizL7KTwZYFnlorX2irHo8oxRAHJdOoX2yTzRqf/A0U/2G0IZ+osohDmEqCKM0UUxCzB7Cbuby/7qkkKDYDNba83araH0Zl6yPYjRIuO/3HRxyg3w0cqj/VUYOOm+qF80P+T8OqnA22cqTupWx7/JBzinKf1bN9MLJ9UR9JriJ5MS/fsbt3PqNB15OtRSbsC72C23MUSFe3vbfubzYbtuxUNR6s4Ux07s2u76IUptN3uAta0Xbvq5TBgRsnzE/Ed+6JCF6dLxelR4h7bVZTtkwWm6//BG4ENgKtt7ynpadRtbTUISaGu+ZWk9WzfOEq2h9GZWrFEEARMLa8p2ytUHmtNoNdBexR4tu0HK8yW76tVoTEekp5FaBttSYQqLwE+aPv2yuN8m4gUbkt88d8IXFHL/gAiX3sSOVlfsD1T0d7nexXtT5EklwTEkm+0ZC3j5YL7RSKSs4Ok9YCmPUtbZoX/S5RoZgeOFLaPkXQW0YRVwCdsN0myH6k93iih2c2mL1I05f4RPX3DRqEicSGi6/9B0+nhsbLEfgdQcxl3EJJCXXMC4fT8mcrCml3aHroEdEnHTXTcdpVmx2WsTwO7EM0PIaIWpxMRpaPcQoF7EAmHks4jcn+aXoO7ExpH21ce51rb6/c8Lwf8yParKo7xpMiXK3TyHgTl4rsW8G3Cqd0X+IPtAyvZP5uQCfmkQ7ZjcWJm21qBXtLjzNY/EyHy9wCVRCl7HIUxSUdhcIUMydwZ539Rs5jk34FPEALRBxLimjNq3dc0AEmhrpH0e+AAIpeyyWuqUlncqe1hc6YGjaSXEJEdEZou1ZsHd5VwqI77qfXYbFqBXAa8HvgLcL3t51WyP2bky/ZeE75x7nYHoqGkkHPYh1DgFpFYfbQr6U1JutL2ppqzFUv1/3MX9NyclibyE68h/kbrA5fb3mq89ybJwoSiwulZtv9QttcCprqSNmC5Dm1OpKh0JinUNZIu7GoC0aXtYVzmm8UgohW2fy3ptmYMSWvavq2W/Y4TDu9SKLWfWrZ3JRyd2pwpaUWiuu4qwjGpuc6/RU/k62BJh1OhuoLZrVH+qYKtcbH9BJHkWbP9Si/3KxopN8uIm1NB3XsQNBWPpTL3PS4NRssE48OTeW7DRn91cWGUqotHHoVMzmeYXQ13MXCIK7QDs21JPyHkTXCFZvd99p+QdLhDoLO6ePMAuUkhl3MGcy611rgndGZ7aJ2prvN0yhg7EUt6zyTWrtcEbiIcuFp0mXD4LuAbzG4BcmnZVxXbnysvfyjpTKJlQc2b+YPl+QFJzyQcwue0NerSm69GCHcsBhX5IsLSpwPrKBSOVyU+V6PEC93Tqd329ZKGPrI2YLqsLk7mjWOB6wm5GYhquOOIiHwNLpO0qe0rK9nr51xJbyDSMEZ12WkZwtHpTSOpIl/Qpe2hXeYbUJ7ONUTPpfMd4pfbEiXt75nLW+dnjCtsbyZpOuEY3kcskdV02DqhJ/l/TCrNFJrctSOJZbJvUiJftj9dyf59PNnZuYfob3hgqYJcELudq4f3hO6vYHbo/rceId0YAEmnErlZ3yP+F7sTSve7TuqJDRGSfga8oae6eDmiungXIjrVVbl4Uug6dULSjUTT7FuJ70PN5OrmWrcs0fD+oR77NRtyJ2MwtJEpOopW9PGo7b9ImiJpiu2LJP1r5TF+XZbIvkusZf+dShG2AVTzvXaCY7VmCoOIfB1BtFc4hbi4vBV4OvBbYia6zYIYbSJfwHttz9E0uXyO2jZS7g/d39DW3iSyJyEm2Cy9TqO7ZdFRpcvq4mTeeFDSVrYvgVking/O5T3zww4VbT0J28t3ab9LJH3U9mGa3atyDtyiV2uXthuG2ZnqOk8H4G9l9jcNOFnSHYRHX4WScHioQ8zx25LOoWLCIRF+PgV4U9neveyrUs1Xs3JyLCaKfEmqFvkCXm37pT3bR0m6zPYhkj5Rwf72PNlx2mGMfQvKyIfubT9ELEf/29x+dhHmFGIZqLe6+FSFFlcXmjvJk/ln4ISSOyXgr0RHjlp83vbbe3dIOolYTlxgJK1GVAk+l8iX+pIrNUMfIL8pz9WLwDq2DQzxMl8viobHtaMVlIvUg8AUYDdCx+pk29WSuCVNt71JLXt9trsOSR9AiNgd07f//cBitr/a0v5EMhi2XSX/S9KviJv4f5ZdbwQOsL15m7+XpH8G3gusA/y+59DyhCr97i1Ou3eckQ3dDzCvbKFA0iZEL9LOqouTuaPQgKK2Q6K+nq0KTbrr2i7hlon6dCIw8E/A8rb3aGNzMih/jy/Zrq4916VtGEJnalB5OmOMuwrwl9ozf0nfBI7vIuFQ0vnA8cxZzben7VdUsn89oRD/SN/+pYArR+VGqBCu+xohdAnROuVDwP8CmzQh/QWwuwKwEnAocFDPofts/3XBz3jhYRB5ZaOOpKmO3pQrj3U8P0vdI2l3298bp6IS20e0tP9xInLU6LhBOMyPAN+1fdB4751H+3NMCvudtlEipRHq0eTprAZsAVxYtrclGh7XaHa4OfAlIoT7OUL0chVCyfodts9pO0YP2wL7SOoi4bC3ms/AL6lbzed+R6rsfLgsYbai68hXQ0kwHy//a4EcqWL3HuAeSZ8C/lz+LtsA60s6sSzvtkZjC1/eA9zq0jh4WBmvorLMEt9KJOIu6pxCRBOmE99j9T1no+PuWbY8d5JzZPtQ4FBJh9r+eAdDSNETtLkuL9a7PWIO+dWKvranMVtQuFYgpTPbQxeZaiiJyHs3F2NJzwC+6Qqq4pJ+TcwSVgCOAnawfZmkFwKnuggj1mCUZ+RlaeaVtv+vb//TiArIVgrcg4p8dZ2oL2kGUdK+FtF4+nTgBbZ3rGT/MmBjQrUX4MWE+OVTgX1dOt4PI2W5ZD+i0fHpwHnA+wiNqRm2XzeJp5ckA0XSXr2TxzKp+JTtg1vavYVQ9B5rkmvbI+OQj5P+USXto0vb2B7KByEf0Ls9Bbihku0ZPa9/03fs6sq/x0nzsm8+bS4NvBPYifjyfBQ4k1jKWqXiub+DSNjbmpixLU9Uvl0BvLOC/esW5NgCjHMeUU22eHnsAZxX0f5V5fmjwPtrf46A7wP/0LO9HlFosHbvZ3kYH0SrpuMJhfgflP/FxcCGk31uw/Yo3+XdgU+X7TWBzSb7vBalB9GQeyrRCP0CovH37hXtn0KINz+DmBRdCXxlsn/vfLR/DOMyX8PPi+7KqUQ04a3Eh7sGT/S87i97rR2qm0NPqsxE2iakn0iUTS9L9He6nlju24q4cVVR/LZ9oqQ7gUOAFxF/mxuAz7hSZ3VJT/PYka+arGq7d0ZyvKT9K9p/VNKuhPPZLCcuUdH+C23PkkWwfaOkjWzfXGG1tWvWdolgSjqauDmt6Q6aKS8E/DtxbdqOSD+4D/ghsOlkntQixqtsf1TSLsDtRKX0RVRqjG77bZLeQkSZHyB0DS+tYbtB0upET75Z93fb02qO0SWSTiBWDv5WtlcCDnedyFRntofWmbL9vvKBbmT9fwXUusluIOleYia4THlN2V56/LfNO70Jh332HyE0p9qwnu0XKRre3m5767L/HIUQaTVsny3pPvclaUvassJF4MvAf0k6kJC/gHA0DwO+0tJ2L1233dmTaG78BdszJT2HShffwm8lfYuIUAG8BfhdWQ4ddvHOWedn+3FJM9ORGpeX2t5Y0tUAtu+WtORkn9QiRjMJ2pFI+fhrzQmLpOcRWms/BNYF3q7oufnAxO+cZ/v/SlwfbgSa3qAmqvxGhfXdk29avge1Um86sz20zlRhJlGB9eby+oc1jNperIaduYzRZcLhI2WMxyT9se9Ylea6fXydyNnp5cgx9s0Xg4h8FcZK1K+moWX7RuADPdsziQKHWuxBSDDsTymZJ3KOHiUKHIaZDfomE8v0TGTsEZB3GCCPlsh104NxVeaMoifdc4akm4gVi/eW/8FDNe0D+9m+oBTxHEAs9dXqiLEzka85yiKvUyStZPtugFLlWstX6cz20DlTkp5PLOk10YP/IBLlh/2mMR69+kO1Eg6fJenrxA2peU3ZXr2F3TmQ9DKionLVvpLhqUAVh7TjyFczxm1Eflmv/f2BtjpZA9FQsv0g0UPy8DEO/73GGF0xiInLQsTXgR8Dq0n6AqGH9qnJPaVFC9sHlejOvSWSej9Qs0hiMxftKtsGDi/VZbW4mYiujbIzdTjwS0mNLuCbgC8Mu+2hq+aT9ATwC2Av278v+272CFUj9KLoUL0isBchv3AscLHtD7ew+c6Jjts+YUFt942zNZFwvi/w7Z5D9wFn2P7vSuM8SROla50USbfZXrOljYFoKI3jrDW9BT/viiKzyeRSKopfQUyMLrD9m7m8JamApO1sX6hxdA7dsnRepZ1Jef0m26f1HPui7RqdGJD0Q2ADIr94lkPlCu1SBomk9YjcweZ7UK0DQFe2h9GZ2oWITG0BnEPkiRxtu3ZfvoFREg6/SeWEw/4v5Xj7Kozz7MYxUDTeXc4VlIF7Il/7M2ebkanALrY3aDvGBGP/wfYaXdmviaTDiOXbU8qutxIXgnuArWxP1EMxSZK5IOlg25/pqnS+d3LYP1GsOXEcb6Jda4I9CCStQ+QCz9LtA6ro9nVqe9icqQZFq5edieW+7YATgB97iDV1xqIkHJ5AVG+sSyQGHlAj4XBQEZ0SXduXuKFPJ/S5jrD95ZZ2BxL5Gmfs1pGpHlv3MX7k6ECHaGgb+5fa3nKsfZKuc0u9r2Ry6fv8qOf14sCStocuHSOZP0qS+Ub9r8faXtTRnLp95xB5ZlV0+7q0PbRfUtv3AycTDYhXJtY2DwJGypmig4RDSTsQ1Sar9+RLQUR0ulDEXs/R7mI3QiPlY4RT1cqZsn0xcLGk4zuKfI3l5EBJhG5rv4cjgD8SkSMRkaOnA78llnW3aWl/OUkvtX05gKTNgOXKsaFWQE/mju05VLclLU8UHOxD5FAlA0LSF4HD+krnD7TdNnfN47wea3uBKZP3QwktulmV6SOWJvNEKa56PfA120c2Fa7DbHtKDSNdY/uvtr/jjnrqdMxmti+AiBXbPpyIuLXhj0TU4yHCqWkepwP/r6XtsVhC0hLEef/U9qPU1eM6VNLUEo28kZACaN2M0vbytqeO8Vi+8mz/1eXzeZ/te20fBexo+z+I3n1teTdwtKSZCqXjo4G9y9/r0Ar2kyFA0oqSPkuo2y8PbGr7wMk9q0WOHdxXOk9MXNuygaR7ywRv/fK62a4ZWT4O+BYxydqW0CQ8qaL9QdCr23dm2VdLt68z2yPhTI0ikj4K0XVc0pv6Drcqy7d9TVkDf67tE3oeP2pKPivzHeAWQiR0Wkm4rtlNfb0SidqZiHytCby9ov2ueULSmyVNKY839xxr7XTavrIs5W1IKIevT/Tlu9/2D9raTyYXSatIOpTQWnsM2Mj2p7KwYFJYTKHfBoCkZYClJvj5ecL2Yr0Tub6JXU2B32XK5F22b7X9WSJNZpTYk5BE6kK3rzPbQ5szNeoMIuFQ0pbAZ5mtdtto93Qe0pW0uCs12ZV0A+EonAJ8w/bFkq7pMgG9JpLWJlr5vKzs+hXwIeB/gU3cJ/vQYpwVgDcAbwPWtV1NBiOZPEr5/Z1EVOFJgqa2jxj4SS2ilEnwTsT/woRG3elNJd6wI+lS4B+B/wQuJK5BX7L9gkk9sUWAoc2ZWgjQOK/H2l5QjiFu2tPpRqwTmNXe5YvAM23vUEpLX1bGr0ET+bqGbiJfnVISzMerqGvlSJWZ8U6EA7UxsfyzM6OlaJxMzJeZHcFcvu9YznYHiO3DJF0LvJK4Tn/O9s8m+bTmh/2BpxAiwp8jolITSukMC+pQt69L27PGyMhUNwwoMnW57Ze2tTMP45xNzNQ+aXsDRRubq7usIqsZ+eoaSc8iFOG3JL6olxD9n25vafdkop3SuYREyIXA7z3CMiHJk5H0rPE+K5Jea/uMQZ/TokyZzD3P9vmSngIs5myB1DnqULevS9sNmTPVHYNIOLxI0pclvUzSxs2jku1eVim5OU9AtLGhYiRM0tMkHVOctkZUbSRmU4XjiOT/ZxIK9GeUfW15EXA38BvgJtuPk5GKhZELJK0ElCrUAAATXUlEQVTVv1PSnrRU6U/mD0l7E0tk3ym7Vgd+MnlnNG9I+mp5PkPS6f2PyT6/ecH2n8rLe4DVyuNvJferlbPTpe2GXObrCA+mjUYTlXpJ79DUTzi8X9JTmd0zbHPiQ1mL4ymRr7L9O6KNUK1lxK5Z1Xav83S8ol1NK0oU8IXEEt/5ku4Alpf0dNt/bms/GRo+BJwnaUcXbTVFo/S3AVtP+M6kNvsBmwGXA9j+b0mrTe4pzRNNxV7NBvEDRdHU+ygijWEmscz6bEk/Bva1/cgw2m5IZ2qE8eD6FR5ARF7WKQmOqxJ9w2qxiu0flBtI08C5sxywDrhL0u7AqWW76SvZGts3Af8C/IukTQkNqysk3W57ixpjJJOL7bMkPQycLWlnQgpjU+DlHVXnJuPzsO1HpEhrLSkNQx8Ntj29PF/c7CsaWWvYvnbSTmz++BQhU7BGs6xaNNe+CXy6PIbRNpDLfCPNWMtjkvaqPMYUQvxta6L1yz7AP1T+gnYd+eqadwFvBv4M/IlwNFvJX4xFkUg4kKje/Hht+8nkUcrZ9wB+DqwNvCIdqUnhYkmfAJaRtD1wGrFsPxJI+rlCs29loqDnOEmjUg36emDv3vy08vq9wC5DbBtIZ2rUOR74GZGrA7E81np5qRfbTwCH237M9g22r3eIdtakP/J1IvD+ymN0hu3bbO9ke1Xbq9nemfjyVkHS8yVdIOn6suvFRPlzshAg6T5J9wJnE10MXgHc0bM/GRwHETIV1xETx7OIqMaosELR7Hs9cJztTYjKxFHgCY/RZs3232kfHezSNpDLfKPOoJbHzpX0BuBHrlz+2Rf5egGxlv3bDhy2QXMA9ZKHvwt8hJIUa/taRb/Ez1eyn0wi7msnk0wetp+Q9BPgJ7bvnOzzWQAWl/QMIlL+ybn98JDhsjQ5lnTQE0NsG0hnatQZ1PLYAYT6+eOSHmS2OOjUtobLxetw2y8Dbmhrb4iopSUG8BTbVzR5HIWRkI1IklFA8eX6DPA+4rurMjE90vYhk3py88chxGrFJbavLILCnTWMr8wKhGbiWNfOtpP4Lm0D6UyNOl0nhgMDmTl3FvmaRGr+HndJWofZTvMbidysJEnqsD+hE7ep7Zkwq7PBtyR9yPa/TerZzSO2TyPyvJrtm4muCUOP7bWKU7uG7dtq265pbyxStHPEKdUmnS+PSdqJEJAE+LntMyf6+fm0fR8l8gVUjXx1STnvsb5AInpkVZmslIv6UUQBwN1Eae9utfRRkmRRR9LVwPa27+rbvypwru2NJufM5g9JhxHL/w8C5wAbAPvbrtXbrnMkTS+5Xl3Y3gW40PY9ZXtFYBvbrbXE0pkaQSRtZ/tCSWMmOdv+UeXxvkSUap9cdu0KTLd9UM1xkrGRtJjtxyUtC0xJNeYkqYuk622/aH6PDRuSZtjesDgNOxMaZhd5RPqcAkj6JnC87Ss7sD3D9oZ9+66u4SznMt9osjXRWmSsfnAGqjpTwI7AhqWyD0knAFcTlS9V6DLytRAwU9I5hJDphZN9MkmyEDKRaGNrQccBskR53hE41fZf+3ItR4FtgX0k3Qrcz+yVitb98xhbwaDOCkJGppK5oWj8uY3tv5btlQmHp8aHOyNfc0HR7Pi1hGDnxsCZwPdtt2qinCRJUJLN7x/rELC07SXGODZ0lGvpzsQy32bAisCZHkAP11qow/55ko4F/kaIdZqQ4FnJ9h6tbaczNXpIOmCi47arirRJ2hX4EnARcXF5OfBx29+vZP9a5ox8LUY0Uq7irC1MlPLerxE5U4NoWZQkyQhRrhH3ltSApwBTR7H9VGnjs3SzXSMpvaRKfJrQ3hLRRP7ztsdypOeLXOYbTZrquhcQEZ2mkeVrgWm1B7N9qqSfl7EEfKyDL+eKwF/L6xUq2x55JG0NvAXYAbiS0JFJkiSZhaR39LzuPXTi4M9mwSgpH4cTYtR3EB0ffgP8Q1vbxWnqZMUjnakRxPbBAJLOBTbu6TX0WXrKYtsi6X22v1E2V7bdVffxQ4GrJc0R+eporJFD0kxgBvAD4CM1ZlFJkiyUbNrzemlCTf8qRsiZAj4HbA6cb3sjSdsSqR8LjKSv2t5f0hmMUYFte6c29iGX+UYaSTcBG9h+uGwvBVxj+4WV7F9le+P+111QVHubyNfloxiW7gpJU0uLiCRJknlG0grASTWchUEh6de2XyLpGmCjIux8he3NWtjcxPb0EuF/Eu5pEL2gZGRqtDkJuELSjwlvexe6m4FULwkZYORrJJH0UduHAZ8fqyLH9gcGf1ZJkowQDwDPm+yTmE/+Jmk54BfAyZLuoGXHB9vTy8sNbX+t95ikDwKtnamMTI04kjYBtiqb02xfXdH2zcCBRDnpYUR/uFm01bMaZORrFJH0WttnSHrnWMdtnzDoc0qSZHjpW8aaAqwH/GCUKqNLkviDxPnvRuTQnmz7LxVsP+k+U0tnKp2phYAuqh6K3eMmOGzb72ppv9eZqvKBXtgpjaGXy2W/JEn66VvGegy41fbtk3U+C0qRR3ie7fNLReJibcSKS0X624jAwy96Di0PPG77la1OmFzmG2nGqHpYE7iJClUPALb3rGFnAlYsSr1TgKn9iu61ldxHFUmnAPsS7XamAytIOsL2lyf3zJIkGSZq5P5MNpL2Bt4DrAysA6wOfJtIpl9Qfkn0M12FuGc23Adc28LuLDIyNcKUBL3t6Kt6sP2eyuOsCLwDWIseB7xtzk7Xka+FhZ4WEbsBmwAfI0RNU4crSZJZSNocOBJYF1gSWAy4f9j7nPYiaQYhOHp5s1oh6TrbL57cM5uYjEyNNo/a/oukKZKm2L5I0r92MM5ZwGXAdcATtYwOIPK1sLCEpCUIZeNv2H5UUs6CkiTp5xtEp4TTgJcQk+DnTuoZzT8P236kKbqRtDhjN5Sfb7p0NtOZGm2aqodpVKp6GIelbU+out6GriJfCxHfAW4BrgGmlXyCzJlKkuRJ2P590xwdOE7SLyf7nOaTiyV9AlhG0vbAe4EzKtnuzNnMZb4Rpsuqh75xPgT8negJ93Czv+nVV8H+Lxkj8pXVauMjaXHbXTjOSZKMKJKmEa1SjiFyhP4E7GF7g0k9sfmgFNnsBbyKkOT5GXC0KzgrPRpW1zZpEpJ+aXuL1rbTmRpNSv+6n9WoQpiHsfYDvkA0iGw+MLa9diX7KYswAUUH5TgiWfJoYCPgINvnTuqJJUkyVJSo9f8RS1gfAqYC37L9+0k9sSGhx9k8GvgzFZ3NKW0NJJNDCeE+UBRuu+YA4Lm217L9nPKo4kgVTpK0t6RnSFq5eVS0P+q8q0ghvApYFdiTaDydJEmCpNdJ2s/2rbYfAs4D9iCEnDec1JObTyRtKek8Sb+TdLOkmUXzsAZvJ/ye9wH3A2sAb6hhOHOmRpuHgOsknUd8MIBOco1uIJR0u+IR4MvAJ+mJfAE1HbZRppE/3xE4zvY1GksSPUmSRZWPErlADUsRlb/LEVHt/5yMk1pAjiGiatMJOZgqlNWcL9jenbh3HlzLNqQzNer8V3nAbCeki5vs48CM0oi4N2eqltPWRL7uqmRvYWN6aWr9HODjkpanYlVlkiQjz5K2/9CzfUnJaf1rya0dJe6xfXZto7Yfl7SqpCVtP1LbfjpTI4ik1wHPsv3Nsn0FsfxjQoOoNj8pj67oOvI16uxFhOpvtv2ApKcSS31JkiQAK/Vu2H5fz+aqAz6Xtlwk6cvAj5hz8n5VBdu3AJdKOp05V3OOaGs4nanRpD+kuyRzhnRPqzmY7RMkLQk8v+z6re1HKw7RdeRrpCld02cCz5e09FzfkCTJosblkva2/d3enZL2Aa6YpHNaUF5anl/Ss8+EQHVb/lgeU4hWMtXIar4RRNKVtjft2f5GMxORdJntzSuPtw1wAuHVi0jae6ftaZXsZyPfCZD0buCDwLOAGcDmwK9s17i4JEky4pT+rD8hJqNNBGcTIndqZ9v/N1nntqiQztQIIun3tscUGpP0P7bXqTzedOBttn9btp8PnGp7k4pjdBn5GmkkXQdsClxW2sq8EDjY9lsm+dSSJBkiJG3H7N6sN9i+cDLPZ36QtLvt70kaUyC6zVKcpDOYQEXd9k4Larshl/lGk0GHdJdoHCkA278r7U2qMFbkS1K1yNdCwEO2H5KEpKVs3yTpBZN9UkmSDBfFeRoZB6qPJlF+rOW3tlGfr5Tn1wNPB75Xtncl7jutycjUCDLokK6kY4kP80ll127A4rV66w0i8jXKSPoxkXC+P5E3cDfh4O44qSeWJEkyACTtb/urFexMs/3yue1bINvpTI0ugwrpSloK2A/YiogcTQP+3fbDE75x3u3PkvafaF8CkrYm2gad00V5b5IkybAh6Tbba1aw8xvgNbZvLtvPAc6yvW5r2+lMJZNN15GvUaVU7u1LNOK8Djgm+/ElSbKoIekPtteoYOfVwFFAo6i+FrCP7Z+1tp3OVDIeJfF5oqS9KpGjriNfo4qk/wAeBX4B7ADcavuDk3tWSZIkg6VWZKrYWgp4Ydm8qdoKSzpTyXiUppkQjg7MGTl6wPYhgz+rRQdJ19l+cXm9OHBFNoROkmRhRNJ9jD15F7CM7SoFc5JeBKwHzNLss31ia7vpTCVzQ9Kltrec274FsDuQyNeoIumqXuepfztJkiSZdyR9BtiGcKbOIiL+l9h+Y1vbKY2QzAvLStrK9iUAkrZgdhlrG/6pPI8Z+apgf9TZQNK95bWAZcq2ANueOnmnliRJMnK8EdgAuNr2npKeBhxdw3A6U8m8sBdwrKQVyvbfgHe1NWr7VgBJW/ZFuQ6SdCmwSC8j2l5sss8hSZJkIeLB0p7rMUlTgTuAtWsYTmcqmSu2pxNRkqnE0vA9lYfoKvKVJEmSJA2/lrQi8F1gOvB3KgldZ85UMldK9cMbiDLSWQ54rQR0SZsAxxL6SVAiX5W6hCdJkiTJHEhaC5hq+9oq9tKZSuaGpHOAewhP/vFmv+3DK4/TVeQrSZIkWcSRNKbSeY3WZelMJXNF0vW2X9Sh/U4jX0mSJElSGh43LA1sBky3vV1b25kzlcwLv5T0YtvXdWT/p8yOfC3SQp1JkiRJN9h+be+2pDWAw2rYzshUMlck3Ui0NJlJODtNaX4tBfROI19JkiRJ0o8kAdc24shtyMhUMi/s0LH9riNfSZIkySKOpCOZLRQ9BdgIuKaK7YxMJfOKpNWYU4L/tkp2O418JUmSJImk/YBGv+8vwC22L61hOyNTyVyRtBNwOPBMQuTs2cBvgH+oNETXka8kSZJkEUXSEsCXgXcAtxAT9tWAI4FLJW1k++o2Y0xpe5LJIsHngM2B39l+DvAKoIo3D6GEXtTQHyRCsM0jSZIkSdpyOLAc8GzbG9veCFgXWFvSt4AftR0gI1PJvPCo7b9ImiJpiu2LJP1rLeMDiHwlSZIkiy47As9zT16T7Xsl/TNwFxVWR9KZSuaFv0laDpgGnCzpDuCxivabyNf5tjeStC2wa0X7SZIkyaLLEx4jQdz245LutH1Z2wFymS+ZF14HPAB8CDgH+B/gtRO+Y/541PZfgFmRL2DDivaTJEmSRZcbJb2jf6ek3YlVkNZkNV8y30haDHir7ZMr2Tsf2Bk4FFiFWOrb1PYWNewnSZIkiy6SVifyoh4kxKENbAosA+xi+39bj5HOVDIepVfefsDqwOnAeWX7I8AM26+rNM6yxId8CrAb0fD45BKtSpIkSZLWSNqOyMUVcIPtC6rZTmcqGQ9JPwXuBn5FVPCtBCwJfND2jA7HrRr5SpIkSZIuSWcqGRdJ1zUy+8XBuQtY0/Z9lewPJPKVJEmSJF2S1XzJRDzavChVDzNrOVKFk5gd+Xo34UQtCbyuy8hXkiRJktQkI1PJuEh6HLi/2SSS9R5gdruXqS3tdxr5SpIkSZJBkJGpZFxsLzb3n2pF15GvJEmSJOmcjEwlk0bXka8kSZIkGQTpTCVJkiRJkrQgFdCTJEmSJElakM5UkiRJkiRJC9KZSpJkKJFkSSf1bC8u6U5JZ86nnVskrdL2Z5IkScYjnakkSYaV+4EXSVqmbG8PtO6hlSRJUpt0ppIkGWbOBl5TXu8KnNockLSypJ9IulbSZZLWL/ufKulcSVdL+g5RHdq8Z3dJV0iaIek7Rd8sSZKkFelMJUkyzHwfeKukpYH1gct7jh0MXG17feATwIll/2eAS2xvRLQpWhNA0rrAW4AtbW8IPE401k6SJGlFinYmSTK02L5W0lpEVOqsvsNbAW8oP3dhiUitALwceH3Z/1+S7i4//wpgE+BKSRC6Znd0/TskSbLwk85UkiTDzunAV4BtgKf27NcYP+u+514EnGD741XPLkmSRZ5c5kuSZNg5FjjE9nV9+6dRlukkbQPcZfvevv07ACuVn78AeKOk1cqxlSU9u/vTT5JkYScjU0mSDDW2bwe+NsahzwLHSbqWaEP0zrL/YOBUSVcBFwO3FTs3SvoUcK6kKURvyP2AW7v9DZIkWdjJdjJJkiRJkiQtyGW+JEmSJEmSFqQzlSRJkiRJ0oJ0ppIkSZIkSVqQzlSSJEmSJEkL0plKkiRJkiRpQTpTSZIkSZIkLUhnKkmSJEmSpAXpTCVJkiRJkrTg/wM/FvEmdyhQewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pylab import *\n",
    "TestModels.ACCU.plot(figsize=(10,6), kind='bar', title='ACCU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DataFrame, Колонка для сортировки, число моделей для извлечения)\n",
    "def get_beast_model(models_df, col, reange_model=1, accuracy=None):\n",
    "    res_list = []\n",
    "    if accuracy:\n",
    "        sort_models_df = models_df.sort_values(by=[col])\n",
    "        for index, row in sort_models_df.iterrows():\n",
    "            if row[col] >= accuracy:\n",
    "                res_list.append(index)\n",
    "        return res_list\n",
    "    else:\n",
    "        temp = models_df.sort_values(by=[col])[-(reange_model):]\n",
    "        #print (temp.head(len(classifiers)))\n",
    "        for index, row in temp.iterrows():\n",
    "            res_list.append(index)\n",
    "        return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модели точность которых выше 90.0 %\n",
      "Name model >>> PassiveAggressiveClassifier:0.9158333333333334\n",
      "Name model >>> MLPClassifier:0.92\n",
      "Name model >>> SGDClassifier:0.9408333333333333\n",
      "Name model >>> GaussianProcessClassifier:0.945\n",
      "Name model >>> LinearSVC:0.9458333333333333\n",
      "Name model >>> LinearDiscriminantAnalysis:0.9466666666666667\n",
      "Name model >>> SVC:0.9466666666666667\n",
      "Name model >>> RidgeClassifierCV:0.9475\n",
      "Name model >>> NuSVC:0.9491666666666667\n",
      "Name model >>> LogisticRegression:0.9491666666666667\n",
      "Name model >>> LogisticRegressionCV:0.95\n",
      "Name model >>> DecisionTreeClassifier:0.9508333333333333\n",
      "Name model >>> RidgeClassifier:0.9516666666666667\n",
      "Name model >>> ExtraTreesClassifier:0.9541666666666667\n",
      "Name model >>> GaussianNB:0.965\n",
      "Name model >>> BaggingClassifier:0.9683333333333334\n",
      "Name model >>> AdaBoostClassifier:0.9683333333333334\n",
      "Name model >>> RandomForestClassifier_default:0.97\n",
      "Name model >>> GradientBoostingClassifier:0.9716666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0.90\n",
    "# Получить наименование всех моделей, точность которых выше 95%\n",
    "beast_models_name = get_beast_model(TestModels, 'ACCU', accuracy=accuracy)\n",
    "print(f\"Модели точность которых выше {accuracy * 100} %\")\n",
    "for name_ in beast_models_name:\n",
    "    prediction = MODELS[name_].predict(X_test)\n",
    "    accuracy = accuracy_score(prediction, y_test)\n",
    "    print(f\"Name model >>> {name_}:{accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 лучшых моделей\n",
      "name beast model >>> GaussianNB:0.965\n",
      "name beast model >>> BaggingClassifier:0.9683333333333334\n",
      "name beast model >>> AdaBoostClassifier:0.9683333333333334\n",
      "name beast model >>> RandomForestClassifier_default:0.97\n",
      "name beast model >>> GradientBoostingClassifier:0.9716666666666667\n"
     ]
    }
   ],
   "source": [
    "reange_model = 5\n",
    "# Получить наименование 5-ти лучших моделей\n",
    "beast_models_name = get_beast_model(TestModels, 'ACCU', reange_model=reange_model)\n",
    "print(f\"5 лучшых моделей\")\n",
    "for name_ in beast_models_name:\n",
    "    prediction = MODELS[name_].predict(X_test)\n",
    "    accuracy = accuracy_score(prediction, y_test)\n",
    "    print(f\"name beast model >>> {name_}:{accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формируем список предсказаний по каждой модели\n",
    "predict_list = []\n",
    "for model_name in beast_models_name:\n",
    "    prediction = MODELS[model_name].predict(X_test)\n",
    "    predict_list.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 0.9741666666666666\n"
     ]
    }
   ],
   "source": [
    "# Методом мажоритарного голосования рассчитываем результирующее предсказание\n",
    "res_prediction = []\n",
    "# redict_list)\n",
    "for i in range(len(predict_list[0])):\n",
    "    prediction = 0\n",
    "    for j in range(len(predict_list)):\n",
    "        prediction += predict_list[j][i]\n",
    "    predict = round(prediction / len(predict_list), 0)\n",
    "    res_prediction.append(predict)\n",
    "    \n",
    "res_prediction = np.array(res_prediction)\n",
    "print(f\"Точность: {accuracy_score(res_prediction, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
